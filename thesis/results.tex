\chapter{Results}
\label{cha:results}

The results of the performance tests conducted are presented here. The chapter
starts with a table describing each test case (Table \ref{tab:test_overview})
and what variable was adjusted in it, followed by sections describing each test
case and explaining their respective outcome. The figures include four series
from the four tested event propagation models and the legend is interpreted as:

\begin{description}

    \item[s/s:] Serial dispatcher with a serial event handler.
    \item[s/p:] Serial dispatcher with a preemptive event handler.
    \item[c/c:] Cooperative dispatcher with a cooperative event handler.
    \item[c/p:] Cooperative dispatcher with a preemptive event handler.

\end{description}

\bgroup
\def\arraystretch{1.2}
\begin{table}[h!]

    \caption[Overview of the performance test cases.]{Overview of the
    performance test cases and their configurations.  The columns with arrows
    ($\rightarrow$) are the variables being adjusted in that specific test
    case.}

    \label{tab:test_overview}
\begin{center}
\begin{tabular}{|r|l|r|r|r|r|r|}
    \hline
    \# & Related figures & Cores $\chi$ & Quantity $q$ & Delay $\delta$ [s] & CPU int. $\lambda_0$ & I/O int. $\lambda_1$ \\
    \hline
    1 & \ref{fig:quad_quantity_throughput}, \ref{fig:quad_quantity_avg_d0},
    \ref{fig:quad_quantity_avg_d2} & 4 & $1 \rightarrow 50$ & 0 & 0.1 & $2
    \times 10^{-4}$ \\
    \hline
    2 & \ref{fig:quad_delay_throughput}, \ref{fig:quad_delay_avg_d0} & 4 & 25 &
    $0.01 \rightarrow 0.1$ & 0.1 & $2 \times 10^{-4}$ \\
    \hline
    3 & \ref{fig:cpu_throughput}, \ref{fig:cpu_avg_d0}, \ref{fig:cpu_avg_d1},
    \ref{fig:cpu_avg_d2} & 4 & 25 & 0 & $0.1 \rightarrow 0.5$ & $10^{-4}$ \\
    \hline
    4 & \ref{fig:cpu_throughput}, \ref{fig:cpu_avg_d0}, \ref{fig:cpu_avg_d1},
    \ref{fig:cpu_avg_d2} & 1 & 25 & 0 & $0.1 \rightarrow 0.5$ & $10^{-4}$ \\
    \hline
    5 & \ref{fig:io_throughput}, \ref{fig:io_avg_d0},
    \ref{fig:quad_io_done_ratio} & 4 & 25 & 0 & 0.05 & $10^{-4} \rightarrow 10^{-3}$ \\
    \hline
    6 & \ref{fig:io_throughput}, \ref{fig:io_avg_d0} & 1 & 25 & 0 & 0.05 & $10^{-4} \rightarrow 10^{-3}$ \\
    \hline
\end{tabular}
\end{center}
\end{table}

After a test has finished, each event has timestamps associated with its
lifecycle in the database. This data is used to calculate throughput, done
ratio, response time, time on device and time on gateway, see Section
\ref{sec:performance_metrics}. Response time is defined as the done timestamp
subtracted with the created timestamp. Time on device is defined as the fetched
timestamp subtracted with the created timestamp. Time on gateway is defined as
the done timestamp subtracted with the retrieved timestamp. The retrieved
timestamp is registered when the dispatcher has read the TCP message on the
socket, not when the message arrived on the socket. This has the implication
that time on device plus time on gateway is not necessarily equal to the total
response time. Some time is "lost" between the fetched timestamp and the
retrieved timestamp.

Each test has a load above 90 \%. This means that more than 90 \% of all
created events in the test were processed and flagged done. The reason for
having a load on this level is that a load smaller than 90 \% will give an
unfair image of the response time, since many events will be queued up on the
device because they are created too fast (see Section \ref{sec:load}). The way
to achieve this was to modify the event frequency parameter $\phi$ so the
gateway was at its peek load while still letting most events be processed. Each
test run was run twice: first with a high $\phi$ value, meaning the load became
smaller than 90 \%, and second with $\phi = \frac{T}{q}$ where $q$ is the
amount of devices in the test and $T$ is the throughput result of the previous
test run. Most of the time, $\phi$ will get a proper value on the second run
and the done ratio will be above 90 \%. However, due to high traffic in the
network and other processes on the testing platform, the resulting done ratio
can be smaller than expected and a new test run must be executed.

\section{Test case 1: Increasing device quantity}

This test was conducted by running the SUT for 30 seconds 11 times, and for
each run increase the number of devices by $1, 5, 10, 15, ..., 50$. Figure
\ref{fig:quad_quantity_throughput} shows clearly that the
cooperative/preemptive gateway is by far faster than the rest for this given
configuration of delay, CPU and I/O intensity (0, $0.1$ and $20^{-4}$
respectively). Each test had the SUT under full load, which means the SUT could
not deliver higher throughput for the given quantity. Due to the restriction
that events from the same device must be handled serially, i.e. even though the
dispatcher is able to pull more events from a device it cannot if a previous
event from the device is still being processed, the throughput is generally
lower for small quantity values. However the throughput stabilizes when the
quantity reaches a certain level.

\begin{figure}[h!]
    \centering
    \performanceplot
    {data/quad_quantity_throughput.csv}
    {
        xlabel=Device quantity $q$,
        ylabel={Throughput [events/s]},
        legend pos=outer north east
    }

    \caption[Throughput result when device quantity was increased.]{Throughput result of the four event propagation models on a quad
    core CPU as device quantity was increased.}

    \label{fig:quad_quantity_throughput}
\end{figure}

The response time scales slowest for the preemptive event handlers, see Figure
\ref{fig:quad_quantity_avg_d0}. There is a correlation between the throughput
and the response time: the two models with highest throughput also has the
lowest response time. The design of the dispatcher is not affecting the
response time; both cooperative dispatch models gives distinct different
outcomes in response time, same for the serial. However, the
cooperative/cooperative gateway has a linear dependency to the device quantity,
see Figure \ref{fig:quad_quantity_avg_d2}, compared to the rest which seem
unaffected by it. The reason is due to two things: the cooperative dispatcher
will fetch events fast as soon they are available on each device and the
single-threaded cooperative event handler will not be able to keep up with the
incoming events, thus they must wait on the gateway. Comparing this to the
serial event handler; each event is being processed directly as soon as it
lands on the gateway because the serial dispatcher will not fetch any new event
before the previous is processed. Therefore the time each event spends on the
gateway is independent of the number of devices. The preemptive event handlers
are able to keep the time on the gateway low, despite the dispatcher design.
This is because they, similar to the serial event handler, will pause the
dispatcher when the thread work queue is full. If the thread pool is configured
to hold four threads, only four events are processed concurrently. The fifth
event must wait for a thread to be free before being dispatched. This wait-task
will pause the entire dispatcher, thus only as many events the gateway can
handle concurrently are dispatched.

\begin{figure}[h!]
    \centering
    \performanceplot
    {data/quad_quantity_avg_d0.csv}
    {
        xlabel=Device quantity $q$,
        ylabel={Response time [ms]},
        legend pos=outer north east
    }

    \caption[Response time result when device quantity was increased.]{The
    average response time of the four event propagation models on a quad core
    CPU as device quantity was increased.}

    \label{fig:quad_quantity_avg_d0}
\end{figure}

\begin{figure}[h!]
    \centering
    \performanceplot
    {data/quad_quantity_avg_d2.csv}
    {
        xlabel=Device quantity $q$,
        ylabel={Time on gateway [ms]},
        legend pos=outer north east
    }

    \caption[Time spent on gateway when device quantity was increased.]{The
    average time each event spent on the gateway on a quad core CPU as device
    quantity was increased.}

    \label{fig:quad_quantity_avg_d2}
\end{figure}

\section{Test case 2: Increasing network delay}

This test was conducted by running the SUT for 30 seconds 10 times, and each
time the network delay was increased by 10 milliseconds. Only the quad core CPU
was tested and the rest of the configuration was set to $q = 25$, $\lambda_0 =
0.1$ and $\lambda_1 = 2 \times 10^{-4}$. Figure \ref{fig:quad_delay_throughput}
shows the throughput result of the tests and it clearly shows that the
cooperative approaches performs much better as network delay is increased.
There is a clear distinction between the cooperative and the serial dispatcher
as the serial/preemptive gateway converge to the same throughput as the
serial/serial gateway quite fast. The serial dispatcher is the main bottleneck,
despite the event handler design. The cooperative dispatcher is able to
concurrently send out TCP requests and it lets the network and the devices work
while the event handler can take care of incoming events. Figure
\ref{fig:quad_delay_avg_d0} shows how the response time depends on the
dispatcher design, not the event handler.

\begin{figure}[h!]
    \centering
    \performanceplot
    {data/quad_delay_throughput.csv}
    {
        xlabel={Network delay $\delta$ [s]},
        ylabel={Throughput [events/s]},
        legend pos=outer north east
    }

    \caption[Throughput result when network delay was increased.]{Throughput
    result of the four event propagation models on a quad core CPU as network
    delay was increased.}

    \label{fig:quad_delay_throughput}
\end{figure}

\begin{figure}[h!]
    \centering
    \performanceplot
    {data/quad_delay_avg_d0.csv}
    {
        xlabel={Network delay $\delta$ [s]},
        ylabel={Response time [ms]},
        legend pos=outer north east
    }

    \caption[Response time when network delay was increased.]{The average
    response time of the four event propagation models on a quad core CPU as
    network delay was increased.}

    \label{fig:quad_delay_avg_d0}
\end{figure}

\section{Test case 3 and 4: Increasing CPU intensity}

This test was conducted by running the SUT for 30 seconds 9 times, and each
time the CPU intensity $\lambda_0$ was increased from 0.1 to 0.5 with steps of
0.05. Both quad and single core CPUs were tested and the rest of the
configuration was set to $q = 25$, $\delta = 0$ and $\lambda_1 = 10^{-4}$. The
throughput result in Figure \ref{fig:cpu_throughput} shows how the preemptive
event handler performs much better than the cooperative and the serial on the
quad core CPU. Both preemptive event handler designs converges as CPU intensity
grows, however it is possible to see the serial dispatcher bottleneck in the
serial/preemptive design when CPU intensity is low. The combination between a
cooperative dispatcher and a preemptive event handler really shines in this
case. For the single core CPU test, it is clear that the operating system
cannot optimize CPU work despite using preemptive event handlers.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style = {group size = 2 by 1},
                width = 0.45*\textwidth,
                xlabel = CPU intensity $\lambda_0$,
                ymax = 125,
                legend pos = outer north east
            ]
            \nextperformanceplot{data/quad_cpu_throughput.csv}{
                title = Quad core CPU,
                ylabel = {Throughput [events/s]}
            }
            \nextperformanceplotwithlegend{data/single_cpu_throughput.csv}{
                title = Single core CPU
            }
        \end{groupplot}
    \end{tikzpicture}

    \caption[Throughput result when CPU intensity was increased.]{Throughput
    result of the four event propagation models as CPU intensity was
    increased.}

    \label{fig:cpu_throughput}
\end{figure}

Figure \ref{fig:cpu_avg_d0} shows how the response time for the
cooperative/cooperative and serial/serial design is independent on number of
cores. The cooperative/cooperative response time scales worse than
serial/serial however. This can be explained by how the cooperative dispatcher
works. It will fetch events as soon as they are available on the devices,
leaving them queued up on the gateway. And as the cooperative event handler
only runs on a single thread, as CPU intensity grows, so will the queue of
events on the gateway. They will therefore wait significantly longer than the
serial event handler. Figures \ref{fig:cpu_avg_d1} and \ref{fig:cpu_avg_d2}
shows where the events are waiting. For the serial/serial design they spend
longer time on the device than they do on the gateway. For the
cooperative/cooperative design its the opposite: they spend longer time on the
gateway than they do on the device.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style = {group size = 2 by 1},
                width = 0.45*\textwidth,
                xlabel = CPU intensity $\lambda_0$,
                legend pos = outer north east
            ]
            \nextperformanceplot{data/quad_cpu_avg_d0.csv}{
                title = Quad core CPU,
                ylabel = {Response time [ms]}
            }
            \nextperformanceplotwithlegend{data/single_cpu_avg_d0.csv}{
                title = Single core CPU
            }
        \end{groupplot}
    \end{tikzpicture}

    \caption[Response time result when CPU intensity was increased.]{The
    average response time of each event expressed in milliseconds as CPU
    intensity increased.}

    \label{fig:cpu_avg_d0}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style = {group size = 2 by 1},
                width = 0.45*\textwidth,
                xlabel = CPU intensity $\lambda_0$,
                legend pos = outer north east
            ]
            \nextperformanceplot{data/quad_cpu_avg_d1.csv}{
                title = Quad core CPU,
                ylabel = {Time on device [ms]}
            }
            \nextperformanceplotwithlegend{data/single_cpu_avg_d1.csv}{
                title = Single core CPU
            }
        \end{groupplot}
    \end{tikzpicture}

    \caption[Time on device when CPU intensity was increased.]{The average time
    each event spends on the device as CPU intensity increased.}

    \label{fig:cpu_avg_d1}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style = {group size = 2 by 1},
                width = 0.45*\textwidth,
                xlabel = CPU intensity $\lambda_0$,
                legend pos = outer north east,
                ymax = 15000
            ]
            \nextperformanceplot{data/quad_cpu_avg_d2.csv}{
                title = Quad core CPU,
                ylabel = {Time on gateway [ms]}
            }
            \nextperformanceplotwithlegend{data/single_cpu_avg_d2.csv}{
                title = Single core CPU
            }
        \end{groupplot}
    \end{tikzpicture}

    \caption[Time on gateway when CPU intensity was increased.]{The average
    time each event spends on the gateway as CPU intensity increased.}

    \label{fig:cpu_avg_d2}
\end{figure}

\section{Test case 5 and 6: Increasing I/O intensity}

This test was conducted by running the SUT for 30 seconds 10 times, and each
time the I/O intensity $\lambda_1$ was increased from $10^{-4}$ to $10^{-3}$
with steps of $10^{-4}$. Both quad and single core CPUs were tested and the
rest of the configuration was set to $q = 25$, $\delta = 0$ and $\lambda_0 =
0.05$. The preemptive and cooperative approaches perform much better than the
serial/serial design and they all perform about the same, especially as I/O
increases, despite the amount of cores, see Figure \ref{fig:io_throughput}.
Because the amount of work performed on each event by the event handler is
mainly I/O oriented, there is no significant difference between the
cooperative/cooperative and the cooperative/preemptive design. This is also why
the number of cores does not affect throughput significantly. The main work is
not done by the CPU, but by the filesystem. This also proves why the response
in Figure \ref{fig:io_avg_d0} is not that different between the number of
cores. There are a number of spikes in the response time measurements that
correlate with the done ratio in Figure \ref{fig:quad_io_done_ratio}. Done
ratio is the number of processed or done events divided by the number of
created events.  When the ratio decreases, there are events waiting to be
processed either on the device or on the gateway. This increases response time
and is the reason why the spikes correlate between the quad core CPU graph in
Figure \ref{fig:io_avg_d0} and Figure \ref{fig:quad_io_done_ratio}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style = {group size = 2 by 1},
                width = 0.45*\textwidth,
                xlabel = I/O intensity $\lambda_1$,
                ymax = 125,
                legend pos = outer north east
            ]
            \nextperformanceplot{data/quad_io_throughput.csv}{
                ylabel = {Throughput [events/s]},
                title = Quad core CPU
            }
            \nextperformanceplotwithlegend{data/single_io_throughput.csv}{
                title = Single core CPU
            }
        \end{groupplot}
    \end{tikzpicture}

    \caption[Throughput result when I/O intensity was increased.]{Throughput
    result of the four event propagation models as I/O intensity was
    increased.}

    \label{fig:io_throughput}
\end{figure}

\begin{figure}[h!]
    \centering
    \performanceplot
    {data/quad_io_done_ratio.csv}
    {table/x=l1, xlabel=I/O intensity $\lambda_1$, ylabel=Done ratio, legend pos=outer north east}

    \caption[Load result when I/O intensity was increased.]{The load result as
    I/O intensity increased on a quad core CPU. This diagram shows the direct
    correlation low values has on response time, see Figure
    \ref{fig:io_avg_d0}.}

    \label{fig:quad_io_done_ratio}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style = {group size = 2 by 1},
                width = 0.45*\textwidth,
                xlabel = I/O intensity $\lambda_1$,
                legend pos = outer north east
            ]
            \nextperformanceplot{data/quad_io_avg_d0.csv}{
                title = Quad core CPU,
                ylabel = {Response time [ms]}
            }
            \nextperformanceplotwithlegend{data/single_io_avg_d0.csv}{
                title = Single core CPU
            }
        \end{groupplot}
    \end{tikzpicture}

    \caption[Response time result when I/O intensity was increased.]{The
    average response time of each event expressed in milliseconds as I/O
    intensity increased.}

    \label{fig:io_avg_d0}
\end{figure}
