\chapter{Implementation}

The be able to reach the goal of this thesis, to map the performance of the
abstract gateway $\Gamma$, a software system has been developed. The ultimate
purpose of this system is to generate and store all events and their respective
timestamps to be able to analyse the performance of $\Gamma$, which processed
them. The system consists of two processes: a \textit{test manager} and a
\textit{gateway} and an analysation tool. The main purpose of the test manager
is to simulate devices and store event data in a database while the purpose of
the gateway is to pull events from the devices and process them. The
analysation tool, hereby referred to as \textit{the reporter}, provide utility
functions to assemble event timestamp information from one or several test runs
available in the database and output them as comma separated values (csv).

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \matrix (table) [layeredblocks] {
            tm      &           &           &       & \\
            conf    &           &           & ls    & db \\
            boot    & ns        &           &       & \\
                    & Device    & Producer  &       & \\
            net     &           &           &       & \\
        };

        \spanblock{table-1-1}{table-1-5}{Test manager}
        \spanblock{table-2-1}{table-2-3}{Configuration}
        \spanblock{table-3-1}{table-4-1}{Boot service}
        \spanblock{table-3-2}{table-3-3}{Name service}
        \spanblock{table-2-4}{table-5-4}{Log server}
        \spanblock{table-5-1}{table-5-3}{Net}
        \spanblock{table-2-5}{table-5-5}{Database}
    \end{tikzpicture}
    \caption{The architecture of the test manager.}
    \label{fig:test_manager_impl_architecture}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \matrix (table) [layeredblocks] {
            gw              &               &               & \\
            conf            &               &               & log \\
            Boot service    & Dispatcher    & Event handler & \\
            net             &               &               & \\
        };

        \spanblock{table-1-1}{table-1-4}{Gateway}
        \spanblock{table-2-1}{table-2-3}{Configuration}
        \spanblock{table-2-4}{table-4-4}{Log module}
        \spanblock{table-4-1}{table-4-3}{Net}
    \end{tikzpicture}
    \caption{The architecture of the gateway.}
    \label{fig:gateway_impl_architecture}
\end{figure}

\section{Communication and protocols}

TCP and UDP technologies are used as communication methods between the test
manager and the gateway. UDP is not connection-oriented like TCP and is able to
send messages with less overhead. UDP is therefore used as medium to send log
messages to the log server, both by the gateway and the name service. Each log
message is sent on a specific format and is explained in Section
\ref{sec:log_server}. The TCP communication middleware is implemented using
\textit{Remote Method Invocation} (RMI) \cite{coulouris2005distributed} and is
placed in the "Net"-module, see Figures
\ref{fig:test_manager_impl_architecture} and
\ref{fig:gateway_impl_architecture}. For the gateway to call an API function on
the test manager (or vice versa), it must produce a JSON-string message on the
format:

\begin{lstlisting}
{
    "name": "hostnames",
    "args": []
}
\end{lstlisting}

If the gateway sends this message to the test manager, the API function
\texttt{hostnames()} will be invoced and its result will be sent back to the
gateway on the format:

\begin{lstlisting}
{
    "result": [5000, 5001, 5002, 5003]
}
\end{lstlisting}

However, if the RMI induce an error while being processed by the API, the error
is returned instead of the result. The error is sent back on the format:

\begin{lstlisting}
{
    "error": {
        "name": "AttributeError",
        "args": ["The API-function is not available."]
    }
}
\end{lstlisting}

The name of the error and the arguments used to create the error is sent back
and enables the receiving end to throw the error as if it was created locally.

\section{The test bootup process}

The test manager and the gateway are run as two different processes on two
different machines. They are started manually from the command line with the
appropiate flags and configurations. Due to the human factor, there is a
possibility that the gateway is started with a configuration different from the
one on the test manager. The test manager can for instance be started with an
I/O intensity value of 0, while the gateway can be started with an I/O
intensity of 1. The test report will then believe the test was run with a
different configuration than it actually was. Another issue is that the
timestamps recorded from the events are set on both machines. The "Created" and
"Fetched" timestamps are set on the test manager machine, while the rest of the
timestamps are set on the gateway machine. If the clocks differ on the two
machines the performance analysis is not trustworthy. It is therefore important
to get the time offset between the test manager and the gateway.

The test bootup process solves these two issues by verifying the configuration
between the two instances and also verify the time offset between them. The
configuration verification is done by letting the gateway send its
configuration to the test manager who checks that the configurations match. If
they don't the test will end with an error stating that the configurations did
not match. If they match, however, the test manager will start a time
synchronization procedure that calculates the time offset between the two
machines. This offset will be added to each event timestamp retrieved from the
gateway. The algorithm that calculates the time offset has been used in video
games earlier and is as follows \cite{simpson2004stream}:

\begin{enumerate}
    \item The test manager saves current local time ($t_{tm_0}$) and requests the
local time from the gateway.
    \item Upon receipt, the gateway's local time ($t_{gw}$) is returned to the
test manager.
    \item The test manager saves the current local time ($t_{tm_1}$) again and
calculates the latency with $t_l = \frac{t_{tm_1} - t_{tm_0}}{2}$. The current time
offset $t_0 = t_{gw} - t_{tm_1} + t_l$ is stored in a list.
    \item Steps 1-3 are repeated five times with a second pause between each
time. This will populate a list with five offset values $t_0$ to $t_4$. The
values are sorted incrementally and the median value is used as the final
time offset value.
\end{enumerate}

Once the configuration has been verified and the time offset is identified, the
test is started by the test manager by calling the "start\_test" API function on
the gateway. The entire test scenario is stored in a database as a table with
four attributes: \textit{scenario ID}, \textit{time offset}, \textit{start
time} of the test and its \textit{end time}. The entire bootup sequence is
illustrated in Figure \ref{fig:bootup_process}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance = 5cm, auto, >=stealth']
        \node[] (tm) {Test manager};
        \node[right of = tm] (gw) {Gateway};

        \node[below of = tm, node distance = 10cm] (tm_ground) {};
        \node[below of = gw, node distance = 10cm] (gw_ground) {};

        \draw (tm) -- (tm_ground);
        \draw (gw) -- (gw_ground);

        \node[draw = none, fill = white, scale = 0.9] at
        ($(tm)!1cm!(tm_ground)$)
        {Waiting for gateway};

        \draw[<-] ($(tm)!2cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Verify config}
        ($(gw)!2cm!(gw_ground)$);
        \draw[->] ($(tm)!2.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Config OK}
        ($(gw)!2.5cm!(gw_ground)$);

        \node[draw = none, fill = white, scale = 0.9] at
        ($(tm)!3.5cm!(tm_ground)$)
        {Start time sync};

        \draw[->] ($(tm)!4.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Get timestamp}
        ($(gw)!4.5cm!(gw_ground)$);
        \draw[<-] ($(tm)!5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Return timestamp}
        ($(gw)!5cm!(gw_ground)$);
        \draw[->] ($(tm)!5.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Get timestamp}
        ($(gw)!5.5cm!(gw_ground)$);

        \draw[<-, draw = none] ($(tm)!6cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{...}
        ($(gw)!6cm!(gw_ground)$);
        \draw[<-] ($(tm)!6.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Return timestamp}
        ($(gw)!6.5cm!(gw_ground)$);

        \node[draw = none, fill = white, scale = 0.9] at
        ($(tm)!7.5cm!(tm_ground)$)
        {Store time offset};

        \draw[->] ($(tm)!8.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Start test}
        ($(gw)!8.5cm!(gw_ground)$);

        \node[draw = none, fill = white, scale = 0.9] at
        ($(gw)!9cm!(gw_ground)$)
        {Start test};
    \end{tikzpicture}
    \caption{Sequence diagram of the bootup process. The test manager waits for
    the gateway to verify its configuration settings. If the configuration does
    not match the one on the test manager, the test is aborted (not shown in
    this figure). If the configuration matches, the test manager starts the
    time sync process that retrieves the timestamp from the gateway a couple of
    times with a 1 second pause between each call. The offset is then stored in
    the database and the test can start.}
    \label{fig:bootup_process}
\end{figure}

\section{The name service}

The name service is a TCP server instantiated and started by the test manager.
Its purpose is to act as the main API available for the gateway to communicate
to the test manager. It also holds references to all simulated devices the
gateway pulls event information from. The name service provide a single API for
the gateway:

\begin{description}

    \item[\texttt{verify\_gateway(configuration, address)}:] Verifies that the
        gateway configuration matches the test manager configuration. The
        address of the gateway API is passed in as well for later use. Returns
        true if the configuration matches, false otherwise.

    \item[\texttt{hostnames()}:] Returns a list of socket ports associated to
        all devices in the test.

\end{description}

When conducting performance tests, the payload data in each event is
irrelevant, only the frequency of events \cite{weyuker2000experience}. The
events are therefore nothing but an identifier. A device is implemented as a
TCP server class with one primary attribute: an event queue. The event queue
stores each event in a FIFO manner (first in, first out). An event is
implemented as a class with a single attribute: its ID, which is a 64-bit UUID
string. The device does not generate events by itself, instead an \textit{event
producer} runs on a separate thread and for a given time interval it pushes new
events onto each device. A class diagram over these components is shown in
Figure \ref{fig:device_class_diagram}. Each device is its own TCP server,
listening to TCP requests on an IP address. The gateway communicates to each
device via its API:

\begin{description}

    \item[\texttt{status()}:] Returns 1 if there is at least one event ready to
        be fetched from the device's event queue. Returns 0 otherwise.

    \item[\texttt{next\_event()}] Returns the next event in the queue. Throws
        and returns an error otherwise.

\end{description}

Note that the method \texttt{put\_event()} in the Device class in Figure
\ref{fig:device_class_diagram} is not part of the TCP server API of the device.
This is because the method is only available locally for the EventProducer class.

\begin{figure}
    \centering
    \begin{tikzpicture}[node distance = 5cm]
        \node[
            classblock,
            text width = 5cm,
            rectangle split parts = 3
        ] (device) {
            Device
            \nodepart{second}
            event\_queue: Queue<Event>
            \nodepart{third}
            status(): int \\
            put\_event(): void \\
            next\_event(): Event
        };

        \node[
            classblock,
            text width = 2cm,
            right of = device,
            yshift = 0.1cm
        ] (event) {
            Event
            \nodepart{second}
            id: string
        };

        \node[
            classblock,
            text width = 4cm,
            rectangle split parts = 3,
            below of = device,
            node distance = 3.5cm
        ] (producer) {
            EventProducer
            \nodepart{second}
            devices: List<Device>
            \nodepart{third}
            generate\_events(): void
        };

        \draw[->] (producer.two west) -- ++(-1cm, 0) |- (device.one west);
        \draw[->] (device.two east) -- (event.one west);
    \end{tikzpicture}

    \caption{Class diagram of the dependencies between the Device, EventProducer and
    Event class.}
    \label{fig:device_class_diagram}
\end{figure}

\section{The log server}
\label{sec:log_server}

The purpose of the log server is to act as a global logging portal for both the
test manager and the gateway and to extract event lifecycle information from
the logs. If the output flag is on, it prints each log message to the same
standard output, regardless of whether the origin of the message is from the
test manager or the gateway. It checks each message for event lifecycle
information and sends the extracted data to the test manager for database
storing. All log messages follow the same format:
\texttt{<level>:<timestamp>:<message>}, e.g. \texttt{INFO:0123456789:Gateway
configuration ok!}. There are four hierarchical, ordered "greater than" log
levels available: \texttt{VERBOSE}, \texttt{DEBUG}, \texttt{INFO} and
\texttt{ERROR}. They are hierarchical and ordered "greater than" in the sense
that if the level is set to \texttt{VERBOSE}, all log messages in the higher
levels are shown as well. If the level on the other hand is set to
\texttt{INFO} the \texttt{VERBOSE} and \texttt{DEBUG} messages are not shown.
Event lifecycle messages are on the format
\texttt{<level>:<timestamp>:<function>:<keyword>:<event\_id>} where keyword is
one of:

\begin{description}

\item[\texttt{EVENT\_LIFECYCLE\_CREATED}:] The event \texttt{event\_id} was
created at time \texttt{timestamp}. The log server extracts the relevant data
from the message using regular expressions and sends them to the test manager
who inserts them in the database.

\item[\texttt{EVENT\_LIFECYCLE\_FETCHED}:] The event \texttt{event\_id} left
the device at time \texttt{timestamp}.

\item[\texttt{EVENT\_LIFECYCLE\_RETRIEVED}:] The event \texttt{event\_id}
arrived at the gateway at time \texttt{timestamp}.

\item[\texttt{EVENT\_LIFECYCLE\_DISPATCHED}:] The event \texttt{event\_id} was
dispatched to the event handler at time \texttt{timestamp}.

\item[\texttt{EVENT\_LIFECYCLE\_DONE}:] The event \texttt{event\_id} was
finished processed at time \texttt{timestamp}.

\end{description}

A regular expression is used both to check whether the message is an event
lifecycle message and to extract the relevant values from it. The log server is
run on a separate thread in the test manager process and listens for UDP
packets. Both the test manager and the gateway sends log messages as UDP
packets to the log server.

