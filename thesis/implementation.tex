\chapter{Implementation}
\label{ch:implementation}

To be able to reach the goal of this thesis, to map the performance of the
abstract gateway $\Gamma$, a software system has been developed. The ultimate
purpose of this system is to generate and store all events and their respective
timestamps to be able to analyse the performance of $\Gamma$, which processed
them. The system consists of two processes: a \textit{test manager} and a
\textit{gateway} as well as an analysation tool. The main purpose of the test
manager is to simulate devices and store event data in a database while the
purpose of the gateway is to pull events from the devices and process them. The
analysation tool is not discussed more than that it provides utility functions
to assemble event timestamp information from one or several test runs available
in the database and output them as comma separated values (\textit{csv}).

The test manager is entirely built in Python with standard libraries. SQLite is
used for the database. Figure \ref{fig:test_manager_impl_architecture} depicts
the overall architecture of the test manager. Each part is explained in the
following sections. The main thread of the program is the actual test manager
that waits for new event lifecycle messages from the logserver, which are
updated in the database. One thread is created for the name service, which is a
TCP server hosting the name service API. One thread is also created for each
device in the test as they are TCP servers hosting the device API used by the
gateway. The producer is also working in its own thread, creating new events at
a given frequency. The log server is a UDP server listening for UDP log
messages in its own thread. The "Net"-module implements the middleware
functionality, enabling communication between the test manager and the gateway.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \matrix (table) [layeredblocks] {
            tm      &           &           &       & \\
            conf    &           &           & ls    & db \\
            boot    & ns        &           &       & \\
                    & Device    & Producer  &       & \\
            net     &           &           &       & \\
        };

        \spanblock{table-1-1}{table-1-5}{Test manager}
        \spanblock{table-2-1}{table-2-3}{Configuration}
        \spanblock{table-3-1}{table-4-1}{Boot service}
        \spanblock{table-3-2}{table-3-3}{Name service}
        \spanblock{table-2-4}{table-5-4}{Log server}
        \spanblock{table-5-1}{table-5-3}{Net}
        \spanblock{table-2-5}{table-5-5}{Database}
    \end{tikzpicture}
    \caption{The architecture of the test manager.}
    \label{fig:test_manager_impl_architecture}
\end{figure}

The gateway is entirely built in C with standard libraries and the libuv
library. The overall architecture of the gateway is shown in Figure
\ref{fig:gateway_impl_architecture}. The boot service is partly a short lived
TCP server listening for timestamp request and start events from the test
manager's boot service. The dispatcher and the event handler have different
configurations, depending on the event propagation model, see Sections
\ref{sec:impl_dispatcher} and \ref{sec:impl_event_handler}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \matrix (table) [layeredblocks] {
            gw              &               & \\
            conf            &               & \\
            Boot service    & Dispatcher    & Event handler \\
            net             &               & \\
        };

        \spanblock{table-1-1}{table-1-3}{Gateway}
        \spanblock{table-2-1}{table-2-3}{Configuration}
        \spanblock{table-4-1}{table-4-3}{Net}
    \end{tikzpicture}
    \caption{The architecture of the gateway.}
    \label{fig:gateway_impl_architecture}
\end{figure}

\section{Configuration}
\label{sec:configuration}

The test manager and the gateway processes must be configurable in order to
test different configurations of the gateway. Both the test manager and the
gateway process are started from the command line and the configuration
parameters are set with flags to the respective command, see Listings
\ref{lst:test_manager_usage} and \ref{lst:gateway_usage}.

\begin{lstlisting}[
    caption = {Usage manual for the test manager process.},
    label = {lst:test_manager_usage}
]
usage: ./run_test [<options>...]

OPTIONS
    -h
        Show this message.

    -q <value>
        The number of simulated devices to run in the test.

    -f <value>
        The frequency in which one device creates new events, expressed in
        times per seconds.

    -l <value>
        The delay expressed in seconds added to each TCP call.

    -d <architecture>
        The architecture of the event dispatcher. Can be one of the following:
            serial
            preemptive
            cooperative

    -e <architecture>
        The architecture of the event handler. Same alternatives as for the
        dispatcher.

    -c <value>
        The CPU intensity each event induce. Value between 0 and 1.

    -i <value>
        The I/O intensity each event induce. Value between 0 and 1.

    -p <value>
        The size of the thread pool. Defaults to 10.

    -t <time value>
        The duration for the test to run. Written in hours, minutes and
        seconds. E.g. 1h2m or 1m10s.

    -g <value>
        The log level to print to stdout. One of 0 (DEBUG), 1 (INFO, default)
        and 2 (ERROR).

    -b <path>
        The file path and name to the database. Defaults to "db".

    -r <value>
        The name of the report to connect this test scenario to. Defaults to
        "Undefined report".
\end{lstlisting}

\begin{lstlisting}[
    caption = {Usage manual for the gateway process.},
    label = {lst:gateway_usage}
]
usage: ./gateway [<options>...]

OPTIONS
    -h
        Show this message.

    -d <architecture>
        The architecture of the event dispatcher. Can be one of the following:
            serial
            preemptive
            cooperative

    -e <architecture>
        The architecture of the event handler. Same alternatives as for the
        dispatcher.

    -c <value>
        The CPU intensity each event induce. Value between 0 and 1.

    -i <value>
        The I/O intensity each event induce. Value between 0 and 1.

    -t <address>
        The IP address (excluding port) of the test manager.

    -n <port>
        The port of the nameservice.

    -l <port>
        The port of the log server.

    -p <value>
        The size of the thread pool. Defaults to 10.
\end{lstlisting}

\subsection{Simulating CPU intensity}

The purpose of the CPU intensity $\lambda_0$ is to simulate CPU intensive work
induced by each event. The intensity value can be set in the configuration to
be a value between 0 and 1, where 0 means no intensity and 1 means maximum
intensity on each event. CPU intensity is simulated by calculating the $n$th
prime number, where $n = \lfloor 2^{12} \times \lambda_0 \rfloor$.

\subsection{Simulating I/O intensity}

The purpose of the I/O intensity $\lambda_1$ is to simulate I/O intensive work
induced by each event. The intensity value is set to be between 0 and 1, where
0 means no I/O work and 1 means maximum I/O work for each event. I/O work can
be any type of work performed by some peripheral unit to the CPU. In this case,
file system operations were chosen. A buffer of size $n$, where $n = \lfloor
2^{28} \times \lambda_1 \rfloor$, is allocated, filled with letters and written
to a file.

\section{Communication and protocols}

TCP and UDP technologies are used as communication methods between the test
manager and the gateway, see Figure \ref{fig:comm_flow}. UDP is not
connection-oriented like TCP and is able to send messages with less overhead.
UDP is therefore used as medium to send log messages to the log server, both by
the test manager and the gateway. Each log message is sent on a specific format
and is explained in Section \ref{sec:log_server}. The TCP communication
middleware is implemented using \textit{Remote Method Invocation} (RMI)
\cite{coulouris2005distributed} and is placed in the "Net"-module, see Figures
\ref{fig:test_manager_impl_architecture} and
\ref{fig:gateway_impl_architecture}. For the gateway to call an API function on
the test manager (or vice versa), it must produce a JSON-string message on the
format:

\begin{lstlisting}
{
    "name": "hostnames",
    "args": []
}
\end{lstlisting}

If the gateway sends this message to the test manager, the API function
\texttt{hostnames()} will be invoced and its result will be sent back to the
gateway on the format:

\begin{lstlisting}
{
    "result": [5000, 5001, 5002, 5003]
}
\end{lstlisting}

However, if the RMI induce an error while being processed by the API, the error
is returned instead of the result. The error is sent back on the format:

\begin{lstlisting}
{
    "error": {
        "name": "AttributeError",
        "args": ["The API-function is not available."]
    }
}
\end{lstlisting}

The name of the error and the arguments used to create the error is sent back
and enables the receiving end to throw the error as if it was created locally.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance = 2cm, auto, >=stealth']
        \node[draw = none] (middle) {};
        \node[draw = none, node distance = 3cm, left = of middle] (hidden_left) {};
        \node[draw = none, node distance = 5cm, above = of middle] (hidden_top) {};
        \node[draw = none, node distance = 4cm, below = of middle] (hidden_bottom) {};

        \node[
            node distance = 0.4cm,
            yshift = -0.1cm,
            left = of hidden_top
        ] {Test manager process};
        \node[
            node distance = 0.4cm,
            yshift = -0.1cm,
            right = of hidden_top
        ] {Gateway process};

        \node[block, text width = 3cm, above = of hidden_left] (tm) {
            Boot service \\
            Name service \\
            Devices
        };

        \node[block, below = of hidden_left] (ls) {
            Log server
        };

        \node[
            block,
            text width = 3cm,
            right = of middle
        ] (gw) {
            Boot service \\
            Dispatcher \\
            Event handler
        };

        \draw[dashed] (hidden_top.north) -- (hidden_bottom.south);
        \draw[->] (tm) -- node[left] {UDP} (ls);
        \draw[->] (gw) -- node[fill = white, sloped, above] {TCP/RMI} (tm);
        \draw[<-] (gw) -- (tm);
        \draw[->] (gw) -- node[fill = white, sloped, above] {UDP} (ls);
    \end{tikzpicture}

    \caption{Illustration of the communication flow between the test manager
    and the gateway.}
    \label{fig:comm_flow}
\end{figure}

\section{The bootup service}
\label{sec:bootup_service}

The test manager and the gateway are run as two different processes on two
different machines. They are started manually from the command line with the
appropiate flags and configurations. Due to the human factor, there is a
possibility that the gateway is started with a configuration different from the
one on the test manager. The test manager can for instance be started with an
I/O intensity value of 0, while the gateway can be started with an I/O
intensity of 1. The test report will then believe the test was run with a
different configuration than it actually was. Another issue is that the
timestamps recorded from the events are set on both machines. The "Created" and
"Fetched" timestamps are set on the test manager machine, while the rest of the
timestamps are set on the gateway machine. If the clocks differ on the two
machines the performance analysis is not trustworthy. It is therefore important
to get the time offset between the test manager and the gateway.

The test bootup process solves these two issues by verifying the configuration
between the two instances and also verify the time offset between them. The
configuration verification is done by letting the gateway send its
configuration to the test manager who checks that the configurations match. If
they don't the test will end with an error stating that the configurations did
not match. If they match, however, the test manager will start a time
synchronization procedure that calculates the time offset between the two
machines. This offset will be added to each event timestamp retrieved from the
gateway. The algorithm that calculates the time offset has been used in video
games earlier and is as follows \cite{simpson2004stream}:

\begin{enumerate}
    \item The test manager saves current local time ($t_{tm_0}$) and requests the
local time from the gateway.
    \item Upon receipt, the gateway's local time ($t_{gw}$) is returned to the
test manager.
    \item The test manager saves the current local time ($t_{tm_1}$) again and
calculates the latency with $t_l = \frac{t_{tm_1} - t_{tm_0}}{2}$. The current time
offset $t_0 = t_{gw} - t_{tm_1} + t_l$ is stored in a list.
    \item Steps 1-3 are repeated five times with a second pause between each
time. This will populate a list with five offset values $t_0$ to $t_4$. The
values are sorted incrementally and the median value is used as the final
time offset value.
\end{enumerate}

Once the configuration has been verified and the time offset is identified, the
test is started by the test manager by calling the "start\_test" API function on
the gateway. The entire test scenario is stored in a database as a table with
four attributes: \textit{scenario ID}, \textit{time offset}, \textit{start
time} of the test and its \textit{end time}. The entire bootup sequence is
illustrated in Figure \ref{fig:bootup_process}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance = 5cm, auto, >=stealth']
        \node[] (tm) {Test manager};
        \node[right of = tm] (gw) {Gateway};

        \node[below of = tm, node distance = 10cm] (tm_ground) {};
        \node[below of = gw, node distance = 10cm] (gw_ground) {};

        \draw (tm) -- (tm_ground);
        \draw (gw) -- (gw_ground);

        \node[draw = none, fill = white, scale = 0.9] at
        ($(tm)!1cm!(tm_ground)$)
        {Waiting for gateway};

        \draw[<-] ($(tm)!2cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Verify config}
        ($(gw)!2cm!(gw_ground)$);
        \draw[->] ($(tm)!2.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Config OK}
        ($(gw)!2.5cm!(gw_ground)$);

        \node[draw = none, fill = white, scale = 0.9] at
        ($(tm)!3.5cm!(tm_ground)$)
        {Start time sync};

        \draw[->] ($(tm)!4.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Get timestamp}
        ($(gw)!4.5cm!(gw_ground)$);
        \draw[<-] ($(tm)!5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Return timestamp}
        ($(gw)!5cm!(gw_ground)$);
        \draw[->] ($(tm)!5.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Get timestamp}
        ($(gw)!5.5cm!(gw_ground)$);

        \draw[<-, draw = none] ($(tm)!6cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{...}
        ($(gw)!6cm!(gw_ground)$);
        \draw[<-] ($(tm)!6.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Return timestamp}
        ($(gw)!6.5cm!(gw_ground)$);

        \node[draw = none, fill = white, scale = 0.9] at
        ($(tm)!7.5cm!(tm_ground)$)
        {Store time offset};

        \draw[->] ($(tm)!8.5cm!(tm_ground)$) --
        node[above, scale = 0.9, midway]{Start test}
        ($(gw)!8.5cm!(gw_ground)$);

        \node[draw = none, fill = white, scale = 0.9] at
        ($(gw)!9cm!(gw_ground)$)
        {Start test};
    \end{tikzpicture}

    \caption[Sequence diagram of the bootup process.]{Sequence diagram of the
    bootup process. The test manager waits for the gateway to verify its
    configuration settings. If the configuration does not match the one on the
    test manager, the test is aborted (not shown in this figure). If the
    configuration matches, the test manager starts the time sync process that
    retrieves the timestamp from the gateway a couple of times with a 1 second
    pause between each call. The offset is then stored in the database and the
    test can start.}

    \label{fig:bootup_process}
\end{figure}

The gateway bootup process starts a TCP server hosting an API used by the test
manager with the following functions:

\begin{description}

    \item[\texttt{get\_timestamp()}:] Returns the timestamp in milliseconds on
        the gateway machine.

    \item[\texttt{start\_test()}:] Signals the gateway that the bootup process
        is finished and that the test can start. The gateway will start
        communication with the devices.

\end{description}

\section{The name service}

The name service is a TCP server instantiated and started by the test manager.
Its purpose is to act as the main API available for the gateway to communicate
to the test manager. It also holds references to all simulated devices the
gateway pulls event information from. The name service provide a single API for
the gateway:

\begin{description}

    \item[\texttt{verify\_gateway(configuration, address)}:] Verifies that the
        gateway configuration matches the test manager configuration. The
        address of the gateway API is passed in as well for later use. Returns
        true if the configuration matches, false otherwise.

    \item[\texttt{hostnames()}:] Returns a list of socket ports associated to
        all devices in the test.

\end{description}

When conducting performance tests, the payload data in each event is
irrelevant, only the frequency of events \cite{weyuker2000experience}. The
events are therefore nothing but an identifier. A device is implemented as a
TCP server class with one primary attribute: an event queue. The event queue
stores each event in a FIFO manner (first in, first out). An event is
implemented as a class with a single attribute: its ID, which is a 64-bit UUID
string. The device does not generate events by itself, instead an \textit{event
producer} runs on a separate thread and for a given time interval it pushes new
events onto each device. A class diagram over these components is shown in
Figure \ref{fig:device_class_diagram}. Each device is its own TCP server,
listening to TCP requests on an IP address. The gateway communicates to each
device via its API:

\begin{description}

    \item[\texttt{status()}:] Returns 1 if there is at least one event ready to
        be fetched from the device's event queue. Returns 0 otherwise.

    \item[\texttt{next\_event()}] Returns the next event in the queue. Throws
        and returns an error if the event queue is empty.

\end{description}

Note that the method \texttt{put\_event()} in the Device class in Figure
\ref{fig:device_class_diagram} is not part of the TCP server API of the device.
This is because the method is only available locally for the EventProducer class.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance = 5cm]
        \node[
            classblock,
            text width = 5cm,
            rectangle split parts = 3
        ] (device) {
            Device
            \nodepart{second}
            event\_queue: Queue<Event>
            \nodepart{third}
            status(): int \\
            put\_event(): void \\
            next\_event(): Event
        };

        \node[
            classblock,
            text width = 2cm,
            right of = device,
            yshift = 0.1cm
        ] (event) {
            Event
            \nodepart{second}
            id: string
        };

        \node[
            classblock,
            text width = 4cm,
            rectangle split parts = 3,
            below of = device,
            node distance = 3.5cm
        ] (producer) {
            EventProducer
            \nodepart{second}
            devices: List<Device>
            \nodepart{third}
            generate\_events(): void
        };

        \draw[->] (producer.two west) -- ++(-1cm, 0) |- (device.one west);
        \draw[->] (device.two east) -- (event.one west);
    \end{tikzpicture}

    \caption[Class diagram of name service components.]{Class diagram of the
    dependencies between the Device, EventProducer and Event class.}

    \label{fig:device_class_diagram}
\end{figure}

\section{The log server}
\label{sec:log_server}

The purpose of the log server is to act as a global logging portal for both the
test manager and the gateway and to extract event lifecycle information from
the logs. If the output flag is on, it prints each log message to the same
standard output, regardless of whether the origin of the message is from the
test manager or the gateway. It checks each message for event lifecycle
information and sends the extracted data to the test manager for database
storing. All log messages follow the same format:
\texttt{<level>:<timestamp>:<message>}, e.g. \texttt{INFO:0123456789:Gateway
configuration ok!}. There are four hierarchical, ordered "greater than" log
levels available: \texttt{VERBOSE}, \texttt{DEBUG}, \texttt{INFO} and
\texttt{ERROR}. They are hierarchical and ordered "greater than" in the sense
that if the level is set to \texttt{VERBOSE}, all log messages in the higher
levels are shown as well. If the level on the other hand is set to
\texttt{INFO} the \texttt{VERBOSE} and \texttt{DEBUG} messages are not shown.
Event lifecycle messages are on the format
\texttt{<level>:<timestamp>:<function>:<keyword>:<event\_id>} where keyword is
one of:

\begin{description}

\item[\texttt{EVENT\_LIFECYCLE\_CREATED}:] The event \texttt{event\_id} was
created at time \texttt{timestamp}. The log server extracts the relevant data
from the message using regular expressions and sends them to the test manager
who inserts them in the database.

\item[\texttt{EVENT\_LIFECYCLE\_FETCHED}:] The event \texttt{event\_id} left
the device at time \texttt{timestamp}.

\item[\texttt{EVENT\_LIFECYCLE\_RETRIEVED}:] The event \texttt{event\_id}
arrived at the gateway at time \texttt{timestamp}.

\item[\texttt{EVENT\_LIFECYCLE\_DISPATCHED}:] The event \texttt{event\_id} was
dispatched to the event handler at time \texttt{timestamp}.

\item[\texttt{EVENT\_LIFECYCLE\_DONE}:] The event \texttt{event\_id} was
finished processed at time \texttt{timestamp}.

\end{description}

A regular expression is used both to check whether the message is an event
lifecycle message and to extract the relevant values from it. The log server is
run on a separate thread in the test manager process and listens for UDP
packets. Both the test manager and the gateway sends log messages as UDP
packets to the log server.

\section{The database}

The primary use of the database is to store event lifecycle timestamps during
the test run so they can be analysed at a later stage. The database schema is
designed to order event lifecycle data with their corresponding test scenario,
see Figure \ref{fig:er_db}. The test scenario, with its ID (sid), is built
around the notion that a test run has a start and an end time and $n$ amount of
events created during the test. The event life cycle has 5 timestamps
associated with it; created, fetched, retrieved, dispatched and done time. Each
event has an event ID (eid) and is associated with a test scenario. Each
scenario has a set of configuration values that are associated with their keys,
which are: \texttt{CPU\_INTENSITY}, \texttt{DEVICE\_DELAY},
\texttt{DEVICE\_FREQUENCY}, \texttt{DEVICE\_QUANTITY}, \texttt{DISPATCHER},
\texttt{EVENT\_HANDLER}, \texttt{IO\_INTENSITY}, \texttt{POOL\_SIZE},
\texttt{TEST\_DURATION}. Test scenarios are grouped into reports. For instance,
if a CPU intensity test is planned where different values of CPU intensity are
run, each test scenario will be associated with a new report that is named
based on the test, e.g. "test cpu 0.1-0.5" if the CPU intensity is tested with
values between $0.1$ and $0.5$.

When a test is about to start, the user specifies the configuration of the test
in the command line, see Section \ref{sec:configuration}. A new record for each
configuration key is created in the configuration table. When the test start, a
new scenario record is created with its start time set.  During the time sync
process, see Section \ref{sec:bootup_service}, the time offset between the test
manager and the gateway is identified and set in the offset attribute in the
scenario record. When a new event is created, the device sends a log message to
the log server, which in turn extracts the relevant data from the message and
creates a new eventlifecycle record with the created\_time attribute set. The
device and the gateway sends log messages as the event passes the associated
lifecycles and the log server updates the eventlifecycle record. When the test
stops the test manager sets the end timestamp in the scenario record and an
entire scenario is stored.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance = 1cm]
        \node[entity, node distance = 2cm] (scenario) {scenario};
        \node[attribute, left = of scenario] (sid) {\underline{sid}} edge
        (scenario);
        \node[attribute, above left = of scenario] (offset) {offset} edge
        (scenario);
        \node[attribute, above = of scenario] (start) {start} edge
        (scenario);
        \node[attribute, above right = of scenario] (end) {end} edge
        (scenario);
        \node[attribute, right = of scenario] (report) {report} edge
        (scenario);

        \node[
            entity,
            node distance = 2cm,
            below left = of scenario
        ] (eventlifecycle) {eventlifecycle} edge node[above, pos = 0.1] {$n$}
        node[above, pos = 0.8] {1} (scenario);
        \node[
            attribute,
            node distance = 0.5cm,
            above = of eventlifecycle
        ] (eid) {\underline{eid}} edge (eventlifecycle);
        \node[attribute, above left = of eventlifecycle] (created)
        {created\_time} edge (eventlifecycle);
        \node[attribute, left = of eventlifecycle] (fetched)
        {fetched\_time} edge (eventlifecycle);
        \node[attribute, below left = of eventlifecycle] (retrieved)
        {retrieved\_time} edge (eventlifecycle);
        \node[attribute, below = of eventlifecycle] (dispatched)
        {dispatched\_time} edge (eventlifecycle);
        \node[attribute, below right = of eventlifecycle] (done)
        {done\_time} edge (eventlifecycle);

        \node[
            entity,
            node distance = 2cm,
            below right = of scenario
        ] (configuration) {configuration} edge node[above, pos = 0.1] {$n$}
        node[above, pos = 0.8] {1} (scenario);
        \node[attribute, left = of configuration] (key)
        {\underline{key}} edge (configuration);
        \node[attribute, below = of configuration] (value)
        {value} edge (configuration);
    \end{tikzpicture}

    \caption{Entitry relationship diagram of the database schema.}
    \label{fig:er_db}
\end{figure}

\section{The dispatcher}
\label{sec:impl_dispatcher}

Two event propagation models were implemented for the dispatcher: the serial
and the cooperative. The serial dispatcher was implemented using standard
libraries in C, see Listing \ref{lst:serial_dispatcher}. This function loops
endlessly over each device retrieved from the \texttt{hostnames()} API function
on the name service. In each iteration a \texttt{status()} call is made to the
device. If the response equals 1, the event is retrieved by
\texttt{next\_event()} and dispatched to either the serial or the preemptive
event handler, depending on the configuration.

\begin{lstlisting}[
    language = C,
    caption = {Code snippet from the serial dispatcher. Some parts has been
    left out in the \texttt{/* */} markings.},
    label = {lst:serial_dispatcher}
]
void dispatcher_serial(config_data_t* config, protocol_value_t* devices)
{
    int i = 0;

    /* ... */

    while (1) {
        net_tcp_context_sync_t* device = devices_context[i];
        int status_ok;

        /*
        make the status() RMI-call and set the result in status_ok...
        */

        if (status_ok) {
            /*
            make the next_event() RMI-call and set the result in device->event...
            */

            // send log messages to the log server
            // these two lines means there is theoratically no time difference
            // between the retrieved- and the dispatched-timestamp.
            log_event_retrieved(device->event);
            log_event_dispatched(device->event);

            // call serial event handler
            if (strcmp(config->eventhandler, "serial") == 0) {
                event_handler_serial(config->cpu, config->io);
            }
            // call preemptive event handler
            else if (strcmp(config->eventhandler, "preemptive") == 0) {
                /*
                wait for thread pool queue
                */

                event_handler_preemptive(device);
            }
        }

        i = (i + 1) % devices_len;
    }
}
\end{lstlisting}

The cooperative dispatcher was implemented using libuv. Listing
\ref{lst:cooperative_dispatcher} shows a snippet of the cooperative dispatcher
function. The serial dispatcher performs socket operations synchronously, i.e.
the entire process is paused while waiting for RMI responses to return. The
cooperative dispatcher uses a state machine functionality, where nodes are
represented as state callback functions. A state callback can make an
asynchronous RMI call and pass in the edge to traverse to when the RMI response
has returned. Each asynchronous RMI call wraps libuv socket operations and
returns instantly. That allows the process to continue despite having socket
transmissions on the line.

\begin{lstlisting}[
    language = C,
    caption = {Code snippet from the cooperative dispatcher. Some parts have
    been left out in the \texttt{/* */} markings.},
    label = {lst:cooperative_dispatcher}
]
void dispatcher_cooperative(config_data_t* config, protocol_value_t* devices)
{
    // create the cooperative state machine
    state_t* coop_dispatch = machine_cooperative_dispatch();

    // just loop each device once
    for (int i = 0; i < devices_len; ++i) {
        machine_coop_context_t* context;

        /* ... */

        // start the state machine for this device, this function returns
        // immediately
        state_machine_run(coop_dispatch, context);
    }

    // start the libuv event loop, this function will not return
    uv_run(loop, UV_RUN_DEFAULT);
}
\end{lstlisting}

\section{The event handler}
\label{sec:impl_event_handler}

Three event propagation models were implemented in the event handler: the
serial, preemptive and cooperative. The serial event handler performs the CPU
work and the I/O work, one after another synchronously, blocking the rest of the
execution, see Listing \ref{lst:serial_event_handler}.

\begin{lstlisting}[
    language = C,
    caption = {Code snippet inspired by the code from the serial event handler
    implementation.},
    label = {lst:serial_event_handler}
]
void event_handler_serial(double cpu_intensity, double io_intensity)
{
    do_cpu_work(cpu_intensity);
    do_io_work(io_intensity);
}
\end{lstlisting}

The preemptive event handler dispatch each event to a thread pool and returns
immediately, see Listing \ref{lst:preemptive_event_handler}. If all threads in
the pool are busy with previous events, the event handler waits for a thread to
be free and blocks the execution on the main thread. The work each thread
performs is identical to the serial event handler.

\begin{lstlisting}[
    language = C,
    caption = {Code snippet inspired by the code from the preemptive event
    handler implementation.},
    label = {lst:preemptive_event_handler}
]
void event_handler_preemptive(double cpu_intensity, double io_intensity)
{
    thpool_add_work(
        threadpool,
        &event_handler_serial,
        cpu_intensity,
        io_intensity);
}
\end{lstlisting}

The cooperative event handler will perform CPU work first, on the same thread,
blocking the rest of the execution, followed by the I/O work that is performed
asynchronously and returns instantly, see Listing
\ref{lst:cooperative_event_handler}. The I/O work will be performed using
libuv's filesystem API. libuv perform filesystem operations in its built-in
threadpool. That means the only difference between the preemptive and the
cooperative event handler is that the cooperative perform CPU work first before
dispatching the work further to the threadpool. libuv's internal threadpool
gets the same size as the number of threads configured when the gateway starts,
but if the threadpool is full the cooperative event handler will not wait for a
thread to be free before the I/O work can be dispatched.

\begin{lstlisting}[
    language = C,
    caption = {Code snippet inspired by the code from the cooperative event
    handler implementation.},
    label = {lst:cooperative_event_handler}
]
void event_handler_cooperative(double cpu_intensity, double io_intensity)
{
    do_cpu_work(cpu_intensity);
    do_io_work_async(io_intensity); // returns immidiately
}
\end{lstlisting}

