\chapter{Implementation}

The be able to reach the goal of this thesis, to map the performance of the
abstract gateway $\Gamma$, a software system has been made. The ultimate goal
of this system is to generate and store all events and their respective
timestamps to be able to analyse the performance of $\Gamma$, which processed
them. The system consists of two processes called \textit{Frontend} and
\textit{Gateway} and an analysation tool called \textit{Report}. The main
purpose of Frontend is to simulate devices and store event data while the
purpose of Gateway is to practically act as a gateway: pull events from the
devices and process them.

\section{The test bootup process}

The test manager and the gateway are run as two different processes on two
different machines. They are started manually from the command line with the
appropiate flags and configurations. Due to the human factor, there is a
possibility that the gateway is started with a configuration different from the
one on the test manager. The test manager can for instance be started with an
I/O intensity value of 0, while the gateway can be started with an I/O
intensity of 1. The test report will then believe the test was run with a
different configuration than it actually was. Another issue is that the
timestamps recorded from the events are set on both machines. The "Created" and
"Fetched" timestamps are set on the test manager machine, while the rest of the
timestamps are set on the gateway machine. If the clocks differ on the two
machines the performance analysis is not trustworthy. It is therefore important
to get the time offset between the test manager and the gateway.

The test bootup process solves these two issues by verifying the configuration
between the two instances and also verify the time offset between them. The
configuration verification is done by letting the gateway send its
configuration to the test manager who checks that the configurations match. If
they don't the test will end with an error stating that the configurations did
not match. If they match, however, the test manager will start a time
synchronization procedure that calculates the time offset between the two
machines. This offset will be added to each event timestamp retrieved from the
gateway. The algorithm that calculates the time offset has been used in video
games earlier and is as follows \cite{simpson2004stream}:

\begin{enumerate}
    \item The test manager saves current local time ($t_{tm_0}$) and requests the
local time from the gateway.
    \item Upon receipt, the gateway's local time ($t_{gw}$) is returned to the
test manager.
    \item The test manager saves the current local time ($t_{tm_1}$) again and
calculates the latency with $t_l = \frac{t_{tm_1} - t_{tm_0}}{2}$. The current time
offset $t_0 = t_{gw} - t_{tm_1} + t_l$ is stored in a list.
    \item Steps 1-3 are repeated five times with a second pause between each
time. This will populate a list with five offset values $t_0$ to $t_4$. The
values are sorted incrementally and the median value is used as the final
time offset value.
\end{enumerate}

Once the configuration has been verified and the time offset is identified, the
test is started by the test manager by calling the "start\_test" API function on
the gateway. The entire test scenario is stored in a database as a table with
four attributes: \textit{scenario ID}, \textit{time offset}, \textit{start
time} of the test and its \textit{end time}.

\todo{Sequence diagram here?}

\section{The name service}

The name service is instantiated and started by the test manager. Its purpose
is to simulate the devices the gateway pulls event information from. The name
service provide a single API for the gateway:

\begin{description}
    \item[\texttt{hostnames()}:] Returns a list of the names of all devices in
the test. The name is a unique 64-bit UUID.
    \item[\texttt{status(device)}:] Returns 1 if the device, identified with
its unique UUID, has an event ready to be fetched. Returns 0 otherwise.
    \item[\texttt{next\_event(device)}:] Returns the next event in the queue of
the device. Throws an error if no event is available in the queue.
\end{description}

\todo{There is an argument in the theory regarding why actual payload data is
not necessary in performance testing, only the frequency of data. Might be good
to use here.}

A device is implemented as a class with two attributes: a name and an event
queue. The name acts as a unique ID and is a 64-bit UUID string \todo{What is
UUID?}. The event queue stores each event in a FIFO manner (first in, first
out). An event is implemented as a class with a single attribute: its ID, which
is a 64-bit UUID string \todo{Some diagram?}. The device does not generate
events by itself, instead an \textit{event producer} runs on a separate thread
and for a given time interval it pushes new events onto each device.

\section{The log server}

The purpose of the log server is to act as a global logging portal for both the
test manager and the gateway and to extract event lifecycle information from
the logs. If the output flag is on, it prints each log message to the same
standard output, regardless of whether the origin of the message is from the
test manager or the gateway. It checks each message for event lifecycle
information and sends the extracted data to the test manager for database
storing. All log messages follow the same format:
\texttt{<level>:<timestamp>:<message>}, e.g. \texttt{INFO:0123456789:Gateway
configuration ok!}. There are four hierarchical, ordered "greater than" log
levels available: \texttt{VERBOSE}, \texttt{DEBUG}, \texttt{INFO} and
\texttt{ERROR}. They are hierarchical and ordered "greater than" in the sense
that if the level is set to \texttt{VERBOSE}, all log messages in the higher
levels are shown as well. If the level on the other hand is set to
\texttt{INFO} the \texttt{VERBOSE} and \texttt{DEBUG} messages are not shown.
Event lifecycle messages are on the format
\texttt{<level>:<timestamp>:<function>:<keyword>:<event\_id>} where keyword is
one of:

\begin{description}

\item[\texttt{EVENT\_LIFECYCLE\_CREATED}:] The event \texttt{event\_id} was
created at time \texttt{timestamp}. The log server extracts the relevant data
from the message using regular expressions and sends them to the test manager
who inserts them in the database.

\item[\texttt{EVENT\_LIFECYCLE\_FETCHED}:] The event \texttt{event\_id} left
the device at time \texttt{timestamp}.

\item[\texttt{EVENT\_LIFECYCLE\_RETRIEVED}:] The event \texttt{event\_id}
arrived at the gateway at time \texttt{timestamp}.

\item[\texttt{EVENT\_LIFECYCLE\_DISPATCHED}:] The event \texttt{event\_id} was
dispatched to the event handler at time \texttt{timestamp}.

\item[\texttt{EVENT\_LIFECYCLE\_DONE}:] The event \texttt{event\_id} was
finished processed at time \texttt{timestamp}.

\end{description}

The log server is run on a separate thread in the test manager process and
listens for UDP packets. Both the test manager and the gateway sends each
log message as a UDP packet to the log server.

