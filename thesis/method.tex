\chapter{Method}
\label{cha:method}

The goal of this study is to map out and understand how IoT gateway
architectures can be developed and how they perform. Also, the asynchronous I/O
library libuv will be studied to see how it can increase the performance of IoT
gateways.

\section{Research and development methodologies}

The study was conducted with Scrum as its backbone. Both the writing of the
thesis and the development of the applications to test happened in two-week
sprints. The project's backlog was initially the fundamental requirements of a
master's thesis (based on requirements from Linköping University) and the
research questions. The forms of its user stories resembled traditional issues
or requirements, but their scopes were wide and their acceptance criteria
abstract. Every sprint they underwent refinement and abstract stories were
split into concrete ones as new knowledge about them were acquired during the
project. For instance, a starting user story (or issue) was \textit{"write the
Results chapter"}. Initially this story was very large, it was hard to do it in
one sprint and it was hard to know where to start. As time progressed and the
implementation to test had been developed and empirical data was gathered, the
story was split into more precise issues like \textit{"present the developed
application"} and \textit{"present a diagram with the test results"}. These
issues were easier to do in one sprint.

The overall phases of the study was a design phase, a technological phase
and a reflexive phase \cite{novikov2013research}. In the design phase, the
concept and goals of the study were developed. Together with the author's own
ideas and input from Attentec, the value of understanding IoT gateway
performance was identified. The hypothesis that the libuv library will improve
IoT gateway performance was stated and a theoretical framework was mapped out
as previous research was studied.

The theoretical framework was developed in this phase to support claims and
form a general direction of the entire study. Multiple databases were queried
in order to find interesting material from previous research. Mainly the online
library hosted by \textit{Linköping University}\footnote{\url{www.bibl.liu.se}}
was used since it allow access to material otherwise unviewable due to
institutional login requirements. Query results from this library is a
collection of query results from other research databases such as \textit{ACM
Digital Library}\footnote{\url{dl.acm.org}}, \textit{ProQuest Ebook
Central}\footnote{\url{ebookcentral.proquest.com}} and \textit{IEEE Xplore
Digital Library}\footnote{\url{ieeexplore.ieee.org}}; so it acts as a gateway
to a global collection of scientific research.

The \textit{three-pass approach} presented by Keshav \cite{keshav2007read} was
be used as a basic approach to find interesting material. It helps the reader
grasp the paper's content in three \textit{passes}. The first pass' purpose is
to give the reader an overview of the paper. The title, abstract, introduction,
headings, sub-headings, conclusions and references are read. This information
should help the reader understand the paper's category and context and help
decide whether to continue read this paper or leave it. If the reader choose to
continue read it, the second pass starts. Here the paper is read more
thoroughly. The figures and diagrams are examined and after this pass the
reader should be able to summarize the paper, with leading evidence, to someone
else. The purpose of the third pass is to fully understand the paper. By making
the same assumptions as the author, the paper is virtually re-created. It helps
identify the true innovations of the paper, as well as the hidden failures.

Together with Attentec, a model of a general, or abstract, gateway was
theoretically developed. The idea was that all IoT gateway implementations have
some common features, constraints and environments that can be simulated and
tested. Another hypothesis developed that different IoT gateway architectures
performed different in different environments, and that there is no
"one-size-fit-all" architecture.

The next phase, the technological phase, started with implementation of the
abstract gateway in order to enable empirical data collection of different
performance measures of different configurations of the abstract gateway. Once
the implementation was considered done, the data collection began.

In the reflexive phase the empirical data was interpreted. There were efforts
in filtering the data and prioritizing what was the most interesting and
significant data that would lead to the best result. The results were then
formulated in this thesis.

\section{Related work}

Previous attempts have been made to prove how certain programming languages
perform better when used in a reactive context. Terber
\cite{terber2017function} discusses the lack of function-oriented software
decomposition for reactive software. With an industrial application as context,
he replaces legacy code with code written in the \textit{Cèu} programming
language, reaching the conclusion that Cèu preserves fundamental software
engineering principles and is at the same time able to fullfill resource
limitations in the system. Jagadeesan et al \cite{jagadeesan1996formal}
performs a similar study but with a different language: \textit{Esterel}. They
reimplement a component in a telephone switching system and reach the
conclusion that Esterel is better suited for analysis and verification for
reactive systems.

No previous research has been found regarding the internal architecture of the
IoT gateway and how it affects performance. Most research regarding IoT
gateways discuss architecture on an application level. Performance testing is
also to a certain degree a non-explored area and most of the literature takes
on empirical approaches, especially for distributed systems performance.

Chen et al. \cite{chen2011brief} describes the IoT gateway as the bridge
between the sensing domain in the IoT architecture and the network domain and
argues its importance as one of the most significant in the IoT architecture.
They describe some common features of the gateway such as support for multiple
network interfaces (2G/3G, LTE, LAN), protocol conversion and manageability. A
reference model is presented where these features are taken into account.
However, the paper is not focusing on the same architectural abstraction level
as this thesis, but provides a more holistic view on the entire IoT gateway
application and sub-applications.

Zachariah et al. \cite{zachariah2015internet} argues that application specific
IoT gateways are not the best approach towards a scalable IoT infrastructure.
The same reason there isn't one web browser per web page, gateways should be
generic and support all types of IoT devices and sensors. They propose a
smartphone-centric architecture where IoT devices connect to the internet via
smartphones. This paper is valuable because it challenges the traditional mind
around IoT gateways. It is however elaborating the gateway on a higher
abstraction level than this thesis is.

Weyuker et al. \cite{weyuker2000experience} states there has been very little
research on software performance testing. In their paper, they discuss
performance testing objectives, workload design and the role of requirements
and specifications to eventually lead up to a case study where they test a
gateway system without having access to historical usage data. They develop
test cases based on usage scenarios to adhere their formulated performance
objectives. The tests are conducted by running programs that simulate clients
in the system, which sends transaction requests in various sizes and
frequencies. The rate of transactions are monitored and used to assess the
performance of the entire system. Their reflections and discussions on
performance testing at large are very valuable to this thesis. However, their
paper focuses on an already existing application upon which performance testing
will be used to assess whether it fulfils its functional and non-functional
requirements. This thesis dives into how an abstract gateway can be modeled and
implemented and how its different configurations affects performance for some
given contexts.

Denaro et al. \cite{denaro2005performance} presents a systematic method to
ascertain that a given distributed software architecture lives up its
performance requirements. They mention some performance metrics such as
latency, throughput and scalability. They state that the overall performance of
a distributed system is often determined by the middleware used, and that same
middleware can affect performance differently depending on its context of
application. This is a good incentive to map and understand IoT gateway
performance, which is done in this thesis.

Kruger et al. \cite{kruger2014benchmarking} discuss the common use of
off-the-shelf components for implementing IoT devices and gateways. They state
little work has previously been done to explore the performance of these
devices when different parameters are adjusted; such as system architecture and
software components. They focus primarily on hardware adjustments (e.g.
processor and memory speed) to identify performance bottlenecks in three common
off-the-shelf brands: Raspberry Pi, Beaglebone and Beaglebone Black. They use
\textit{micro-benchmarks} to identify performance bottlenecks at architecture
level and \textit{macro-benchmarks} at application level. They reach the
conclusion that gateway performance is most important when there is a large
amount of data being transfered between the gateway and the IoT application.

Aguilera et al. \cite{aguilera2003performance} presents an approach to identify
performance bottlenecks in distributed systems where the components are seen as
black boxes, i.e. sub-systems of unknown functionality. By monitoring the
traffic of \textit{remote procedure calls} in the system and store them in a
log they are able to find the \textit{casual paths} of messages and thereby
understand the data flow in the system and where performance bottlenecks are
located. The approach to capture log messages is used in this thesis, however
the step of storing messages in a log file before computation is omitted and
each log message is computed as it arrives.

Liu et al. \cite{liu2002designing} presents an approach to predict distributed
system performance empirically by running test suites. They map out some
parameters that affect performance, such as client request load and thread pool
size and some observable parameters such as throughout and response time. They
reach the conclusion that gathering empirical results with simple test cases is
an effective method to understand the performance profile of the system.

\section{Two customer cases}
\label{sec:customer-cases}

Before presenting the architectural models used in this study, it is of
importance to understand how they came about. Among the customers of Attentec,
two were of interest for this study with each having implemented an IoT gateway
suitable for its operating context. \textit{Customer A} uses their IoT gateway
to monitor industrial batteries used in forklifts. The main idea behind the
event propagation model is that the gateway polls each battery for events and
process them according to its specification (which is unrelevant here).
Referring to data propagation in reactive languages (see section
\ref{sec:evaluation-model}), one can view the event propagation as
\textit{pull-based}, i.e. the gateway pulls events from the batteries whenever
it finds it necessary. The batteries are \textit{passive}, i.e. they do not
emit any events on their own, only when asked to by the gateway. Figure
\ref{fig:pull-based} illustrates this.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[auto]
        \node [block] (device_1) {Device 1};
        \node [block, right of=device_1, node distance=3cm] (device_2) {Device 2};
        \node [block, right of=device_2, node distance=3cm] (device_n) {Device n};
        \node [block, below of=device_2, node distance=2.5cm] (gateway) {Gateway};

        \path [->] (device_1) edge [bend left=5] (gateway);
        \path [->] (gateway) edge [bend left=5] (device_1);
        \path [->] (device_2) edge [bend left=5] (gateway);
        \path [->] (gateway) edge [bend left=5] (device_2);
        \path [->] (device_n) edge [bend left=5] (gateway);
        \path [->] (gateway) edge [bend left=5] (device_n);
    \end{tikzpicture}
    \caption[Pull-based event propagation.]{Pull-based event propagation with
    passive devices. The gateway pulls events from the devices by requesting
    them first.}
    \label{fig:pull-based}
\end{figure}

\textit{Customer B} on the other hand uses a \textit{push-based} approach to
handle event propagation in their gateway. The gateway hosts a REST API to
serve IoT devices. This means that the devices must be \textit{active}, i.e.
they emit events whenever they find it necessary. See Figure
\ref{fig:push-based} for an illustration. Note that not as many communication
requests are necessary in this approach compared to the pull-based one. The
gateway does not need to know about how many devices there are or what their
addresses are, but the devices need to know the address of the gateway. In
contrast, the pull-based approach requires the gateway to know about each
device, but the device does not need to know anything about the gateway.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[auto]
        \node [block] (device_1) {Device 1};
        \node [block, right of=device_1, node distance=3cm] (device_2) {Device 2};
        \node [block, right of=device_2, node distance=3cm] (device_n) {Device n};
        \node [block, below of=device_2, node distance=2.5cm] (gateway) {Gateway};

        \path [line] (device_1) -- (gateway);
        \path [line] (device_2) -- (gateway);
        \path [line] (device_n) -- (gateway);
    \end{tikzpicture}
    \caption[Push-based event propagation.]{Push-based event propagation with
    active devices. Events are pushed from the devices to the gateway that
    listens for incoming requests.}
    \label{fig:push-based}
\end{figure}

\section{Three event propagation models}

Taking inspiration from the three task management approaches presented by Adya
et al \cite{adya2002cooperative} and described in Section
\ref{sec:task-management}, three event propagation models are proposed:
\textit{serial}, \textit{preemptive} and \textit{cooperative event
propagation}. Before presenting a formal definition, an analogy might suite
well here. Imagine a cafe that serves coffee. There are several customers in
line and the barista takes the order from the next customer in line, brews the
coffee and serves it. The coffee brewer can only brew one cup of coffee at a
time. This is repeated for each customer in line with the constraint that no
new order is accepted before the current order is served, see Figure
\ref{fig:cafe-serial}. This can be seen as a \textit{serial order handling};
only one order can be handled at a time. Imagine now that the cafe hires more
baristas and more coffee machines. There is still one line (but there can be
several), but orders can be handled concurrently, so the next customer in line
does not necessarily have to wait for the previous customer to be served before
he can place his order, see Figure \ref{fig:cafe-preemptive}. This can be seen
as \textit{preemptive order handling}. Now lets keep the brewers but let go all
baristas except for one.  All customers place their order to the same barista
which in turn start one of the brewers. While the brewer is working, the next
customer in line places his order and the barista can start a second brewer,
see Figure \ref{fig:cafe-cooperative}. This is called \textit{cooperative order
handling}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=5cm, auto, >=stealth']
        \node[] (brewer) {Brewer};
        \node[left of=brewer] (barista) {Barista};
        \node[left of=barista] (customers) {Customer line};

        \node[below of=brewer, node distance=7.5cm] (brewer_ground) {};
        \node[below of=barista, node distance=7.5cm] (barista_ground) {};
        \node[below of=customers, node distance=7.5cm] (customers_ground) {};

        \draw (brewer) -- (brewer_ground);
        \draw (barista) -- (barista_ground);
        \draw (customers) -- (customers_ground);

        \draw[<-] ($(barista)!1cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_0$}
            ($(customers)!1cm!(customers_ground)$);
        \draw[<-] ($(brewer)!1.1cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista)!1.1cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer)!1.5cm!(brewer_ground)$) {Brewing...};
        \draw[->] ($(brewer)!1.9cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista)!1.9cm!(barista_ground)$);
        \draw[->] ($(barista)!2cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_0$}
            ($(customers)!2cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!2.5cm!(customers_ground)$)
            {Next customer};

        \draw[<-] ($(barista)!3cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_1$}
            ($(customers)!3cm!(customers_ground)$);
        \draw[<-] ($(brewer)!3.1cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista)!3.1cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer)!3.5cm!(brewer_ground)$) {Brewing...};
        \draw[->] ($(brewer)!3.9cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista)!3.9cm!(barista_ground)$);
        \draw[->] ($(barista)!4cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_1$}
            ($(customers)!4cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!4.5cm!(customers_ground)$)
            {Next customer};

        \node[draw=none, fill=white, scale=0.9] at ($(brewer)!5cm!(brewer_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista)!5cm!(barista_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!5cm!(customers_ground)$)
            {...};

        \draw[<-] ($(barista)!6cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_j$}
            ($(customers)!6cm!(customers_ground)$);
        \draw[<-] ($(brewer)!6.1cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista)!6.1cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer)!6.5cm!(brewer_ground)$) {Brewing...};
        \draw[->] ($(brewer)!6.9cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista)!6.9cm!(barista_ground)$);
        \draw[->] ($(barista)!7cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_j$}
            ($(customers)!7cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at ($(brewer)!7.5cm!(brewer_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista)!7.5cm!(barista_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!7.5cm!(customers_ground)$)
            {...};
    \end{tikzpicture}

    \caption[A sequence diagram of a serial cafe order line.]{A sequence diagram
    of a serial cafe order line. Each customer is served one at a time and no
    customer is served before the previous customer receives its order.}

    \label{fig:cafe-serial}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=3cm, auto, >=stealth']
        \node[] (brewern) {$Brewer_{n-1}$};
        \node[left of=brewern] (baristan) {$Barista_{n-1}$};
        \node[left of=baristan, node distance=1.5cm] (dots) {...};
        \node[left of=dots, node distance=1.5cm] (brewer0) {$Brewer_0$};
        \node[left of=brewer0] (barista0) {$Barista_0$};
        \node[left of=barista0] (customers) {Customer line};

        \node[below of=brewern, node distance=7cm] (brewern_ground) {};
        \node[below of=baristan, node distance=7cm] (baristan_ground) {};
        \node[below of=dots, node distance=7cm] (dots_ground) {};
        \node[below of=brewer0, node distance=7cm] (brewer0_ground) {};
        \node[below of=barista0, node distance=7cm] (barista0_ground) {};
        \node[below of=customers, node distance=7cm] (customers_ground) {};

        \draw (brewern) -- (brewern_ground);
        \draw (baristan) -- (baristan_ground);
        \draw (brewer0) -- (brewer0_ground);
        \draw (barista0) -- (barista0_ground);
        \draw (customers) -- (customers_ground);

        \draw[<-] ($(barista0)!1cm!(barista0_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_0$}
            ($(customers)!1cm!(customers_ground)$);
        \draw[<-] ($(brewer0)!1.1cm!(brewer0_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista0)!1.1cm!(barista0_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer0)!1.5cm!(brewer0_ground)$) {Brewing...};
        \draw[->] ($(brewer0)!2.9cm!(brewer0_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista0)!2.9cm!(barista0_ground)$);
        \draw[->] ($(barista0)!3cm!(barista0_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_0$}
            ($(customers)!3cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!1.5cm!(customers_ground)$)
            {Next customer};

        \draw[<-, dashed] ($(dots)!2cm!(dots_ground)$) --
            node[above, scale=0.9, pos=0.4]{Place $Order_1$}
            ($(customers)!2cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!3.5cm!(customers_ground)$)
            {Next customer};

        \node[draw=none, fill=white, scale=0.9] at ($(brewern)!4cm!(brewern_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(baristan)!4cm!(baristan_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(brewer0)!4cm!(brewer0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista0)!4cm!(barista0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!4cm!(customers_ground)$)
            {...};

        \draw[<-] ($(baristan)!4.5cm!(baristan_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_j$}
            ($(customers)!4.5cm!(customers_ground)$);
        \draw[<-] ($(brewern)!4.6cm!(brewern_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(baristan)!4.6cm!(baristan_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewern)!5.5cm!(brewern_ground)$) {Brewing...};
        \draw[->] ($(brewern)!6.4cm!(brewern_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(baristan)!6.4cm!(baristan_ground)$);
        \draw[->] ($(baristan)!6.5cm!(baristan_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_j$}
            ($(customers)!6.5cm!(customers_ground)$);

        \draw[->, dashed] ($(dots)!5.5cm!(dots_ground)$) --
            node[above, scale=0.9, pos=0.41]{Serve $Order_{j-1}$}
            ($(customers)!5.5cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at ($(brewern)!7cm!(brewern_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(baristan)!7cm!(baristan_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(brewer0)!7cm!(brewer0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista0)!7cm!(barista0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!7cm!(customers_ground)$)
            {...};
    \end{tikzpicture}

    \caption[A sequence diagram of a preemptive cafe order line.]{A sequence
    diagram of a preemptive cafe order line. There are $n$ baristas and brewers
    that can handle orders concurrently and customers does not necessarily have
    to wait for previous orders to be returned before they can place their
    own.}

    \label{fig:cafe-preemptive}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=3cm, auto, >=stealth']
        \node[] (brewern) {$Brewer_{n-1}$};
        \node[left of=brewern, node distance=1.5cm] (dots) {...};
        \node[left of=dots, node distance=1.5cm] (brewer0) {$Brewer_0$};
        \node[left of=brewer0] (barista) {Barista};
        \node[left of=barista] (customers) {Customer line};

        \node[below of=brewern, node distance=7cm] (brewern_ground) {};
        \node[below of=dots, node distance=7cm] (dots_ground) {};
        \node[below of=brewer0, node distance=7cm] (brewer0_ground) {};
        \node[below of=barista, node distance=7cm] (barista_ground) {};
        \node[below of=customers, node distance=7cm] (customers_ground) {};

        \draw (brewern) -- (brewern_ground);
        \draw (brewer0) -- (brewer0_ground);
        \draw (barista) -- (barista_ground);
        \draw (customers) -- (customers_ground);

        \draw[<-] ($(barista)!1cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_0$}
            ($(customers)!1cm!(customers_ground)$);
        \draw[<-] ($(brewer0)!1.1cm!(brewer0_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista)!1.1cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer0)!1.5cm!(brewer0_ground)$) {Brewing...};
        \draw[->] ($(brewer0)!2.9cm!(brewer0_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista)!2.9cm!(barista_ground)$);
        \draw[->] ($(barista)!3cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_0$}
            ($(customers)!3cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!1.5cm!(customers_ground)$)
            {Next customer};

        \draw[<-] ($(barista)!2cm!(barista_ground)$) --
            node[above, scale=0.9, pos=0.4]{Place $Order_1$}
            ($(customers)!2cm!(customers_ground)$);
        \draw[<-, dashed] ($(dots)!2.1cm!(dots_ground)$) --
            node[above, scale=0.9, pos=0.65]{Start brewer}
            ($(barista)!2.1cm!(barista_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!3.5cm!(customers_ground)$)
            {Next customer};

        \node[draw=none, fill=white, scale=0.9] at ($(brewern)!4cm!(brewern_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(brewer0)!4cm!(brewer0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista)!4cm!(barista_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!4cm!(customers_ground)$)
            {...};

        \draw[<-] ($(barista)!4.5cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_j$}
            ($(customers)!4.5cm!(customers_ground)$);
        \draw[<-] ($(brewern)!4.6cm!(brewern_ground)$) --
            node[above, scale=0.9, pos=0.75]{Start brewer}
            ($(barista)!4.6cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewern)!5.5cm!(brewern_ground)$) {Brewing...};
        \draw[->] ($(brewern)!6.4cm!(brewern_ground)$) --
            node[above, scale=0.9, pos=0.75]{Coffee ready}
            ($(barista)!6.4cm!(barista_ground)$);
        \draw[->] ($(barista)!6.5cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_j$}
            ($(customers)!6.5cm!(customers_ground)$);

        \draw[->, dashed] ($(dots)!5.5cm!(dots_ground)$) --
            node[above, scale=0.9, pos=0.41]{Serve $Order_{j-1}$}
            ($(customers)!5.5cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at ($(brewern)!7cm!(brewern_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(brewer0)!7cm!(brewer0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista)!7cm!(barista_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!7cm!(customers_ground)$)
            {...};
    \end{tikzpicture}

    \caption[A sequence diagram of a cooperative cafe order line.]{A sequence
    diagram of a cooperative cafe order line. There is only one barista but $n$
    brewers. As soon as a brewer is working, the next order can be handled.}

    \label{fig:cafe-cooperative}
\end{figure}

Looking at all three of these order handling approaches, we find a few common
entities. They all have orders, a customer line, one or several baristas and
brewers. Lets generalize this a bit and call orders \textit{events}, the
customer line we call the \textit{dispatcher}, the barista and brewer combined
we call the \textit{event handler} and the work done by the event handler on
event $\epsilon_j$ we call $\Lambda_j$. The definition of the three event
propagation models is as follows:

\begin{description}

    \item[\textbf{Serial event propagation}:] Given a set of $m$ events $\{
            \epsilon_0, \epsilon_1, ..., \epsilon_j, ..., \epsilon_{m-1} \}$ no
        event $\epsilon_j$ is dispatched before the work $\Lambda_{j-1}$ is
        finished, see Figure \ref{fig:event-propagation-serial}.

    \item[\textbf{Preemptive event propagation}:] Given a set of $m$ events $\{
            \epsilon_0, \epsilon_1, ..., \epsilon_j, ..., \epsilon_{m-1} \}$
        each event $\epsilon_j$ can be dispatched independent on
        $\Lambda_{j-1}$ and the work $\Lambda_j$ can implicitly interleave and
        make room for work $\Lambda_k$ where $k \ne j$.

    \item[\textbf{Cooperative event propagation}:] Given a set of $m$ events
        $\{ \epsilon_0, \epsilon_1, ..., \epsilon_j, ..., \epsilon_{m-1} \}$
        each event $\epsilon_j$ can be dispatched independent on
        $\Lambda_{j-1}$ and the work $\Lambda_j$ can explicitly make room for
        work $\Lambda_k$ where $k \ne j$.

\end{description}

Common for all models is that the work $\Lambda_j$ can not start before the
event $\epsilon_j$ is dispatched. Each event follow the order $\epsilon_j
\rightarrow dispatch \rightarrow \Lambda_j \rightarrow finish$. An example of
workflow of a serial event propagation model is illustrated in Figure
\ref{fig:event-propagation-serial}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=5cm, auto, >=stealth']
        \node[] (event_handler) {Event handler};
        \node[left of=event_handler] (dispatcher) {Dispatcher};

        \node[below of=event_handler, node distance=8cm] (event_handler_ground) {};
        \node[below of=dispatcher, node distance=8cm] (dispatcher_ground) {};

        \draw (event_handler) -- (event_handler_ground);
        \draw (dispatcher) -- (dispatcher_ground);

        \draw[<-] ($(event_handler)!1cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Dispatch $\epsilon_0$}
            ($(dispatcher)!1cm!(dispatcher_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(event_handler)!1.5cm!(event_handler_ground)$)
            {$\Lambda_0$};
        \draw[->] ($(event_handler)!2cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Finish $\epsilon_0$}
            ($(dispatcher)!2cm!(dispatcher_ground)$);

        \draw[<-] ($(event_handler)!3cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Dispatch $\epsilon_1$}
            ($(dispatcher)!3cm!(dispatcher_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(event_handler)!3.5cm!(event_handler_ground)$)
            {$\Lambda_1$};
        \draw[->] ($(event_handler)!4cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Finish $\epsilon_1$}
            ($(dispatcher)!4cm!(dispatcher_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
        ($(event_handler)!5cm!(event_handler_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at
        ($(dispatcher)!5cm!(dispatcher_ground)$)
            {...};

        \draw[->] ($(event_handler)!5.5cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Finish $\epsilon_{j-1}$}
            ($(dispatcher)!5.5cm!(dispatcher_ground)$);

        \draw[<-] ($(event_handler)!6.5cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Dispatch $\epsilon_j$}
            ($(dispatcher)!6.5cm!(dispatcher_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(event_handler)!7cm!(event_handler_ground)$)
            {$\Lambda_j$};
        \draw[->] ($(event_handler)!7.5cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Finish $\epsilon_j$}
            ($(dispatcher)!7.5cm!(dispatcher_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
        ($(event_handler)!8cm!(event_handler_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at
        ($(dispatcher)!8cm!(dispatcher_ground)$)
            {...};
    \end{tikzpicture}

    \caption[A sequence diagram of the serial event propagation model.]{A
    sequence diagram of the serial event propagation model. Only one event
    $\epsilon_j$ is dispatched and handled at a time and no event $\epsilon_j$
    can be dispatched before the previous work $\Lambda_{j-1}$ is finished.}

    \label{fig:event-propagation-serial}
\end{figure}

\subsection{The anatomy of an event}

The event is the actual data transmitted from the device to the gateway.
Imagine a small weather station used at private homes. It has two sensors
measuring wind velocity and air temperature. The gateway is the central system
passing the data from each sensor to the application using the data. At a
certain frequency, the gateway polls each sensor for new velocity and
temperature values. These values are the actual events in the system. In this
toy example, each event might not only carry the actual value of the sensor it
corresponds to, but also information on what sensor this data is coming from.
The weather application might draw a plot on how the air temperate has changed
over time, therefore each event must also include the timestamp of each sensor
value. In this study, however, there is no need for event data that represents
some mimicked sensor value. What is needed is a way to track each event to see
when it reaches certain places in its life. This is done by registering the
timestamp when the event reaches those places, see Figure \ref{fig:event-life}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=3cm, auto]
        \node[block] (device) {Device};
        \node[below of=device] (inv) {};
        \node[block, below of=inv] (dispatcher) {Dispatcher};
        \node[block, below of=dispatcher] (event_handler) {Event Handler};

        \path[line] (device) -- (dispatcher);
        \path[line] (dispatcher) -- (event_handler);

        \node[comment, node distance=1cm, right =of device] (created) {Created};
        \node[comment, align=left, node distance=1cm, right =of inv] (fetched_retrieved)
            {Leaves device /\\ arrives at gateway};
        \node[comment, node distance=1cm, right =of dispatcher] (dispatched) {Dispatched};
        \node[comment, node distance=1cm, right =of event_handler] (processed) {Processed};
    \end{tikzpicture}

    \caption[Over of the event lifecycle.]{Overview of the event lifecycle. The
    event travels from the device down to the event handler. At each step the
    time point is registered, shown in the clouds.}

    \label{fig:event-life}
\end{figure}

An event $\epsilon$ can generally be considered as a vector

$$
\epsilon = \big[ t_0, t_1, ..., t_{k-1} \big]
$$

where $t_i$ represents the timestamp when $\epsilon$ reached place $i$ in its
life. Let $k$ be the number of different places the event can reach. In this
study, five places are registered: the creation of the event, when the event
leaves the device, when the event reaches the gateway, when the event is
dispatched to the event handler and when the event is processed by the event
handler.

\subsection{The dispatcher}

The purpose of the dispatcher is to listen to or poll for events and then
dispatch them to the event handler. As mentioned in Section
\ref{sec:customer-cases}, there are two main approaches to this: pull-based or
push-based. The devices generating the events are either active and can push
each event to the gateway, which in turn has to listen to them, or the devices
are passive and the gateway must pull events from them. In this study, the
pull-based approach is used to mimick the gateway used by Customer A. This
approach requires the dispatcher to have a reference to each device to be able
to pull events from them.

An important question to consider is: how is the dispatcher related to the
event propagation models? If the architecture of the gateway implements the
serial event propagation model, the dispatcher should make sure it does not
dispatch an event before the previous event is dispatched, see Figure
\ref{fig:dispatcher-serial}. Both the preemptive and the cooperative event
propagation models loosens this constraint and allows the dispatcher to
dispatch events before previous events have been dispatched. In contrast to the
serial approach in Figure \ref{fig:dispatcher-serial}, all devices can be
polled for events concurrently. There is no need, in theory, to loop through
the list of device references and check them one by one. Instead, there can be
one dispatch process for each device, see Figure \ref{fig:dispatcher-pre-coop}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=2.5cm, auto]
        \node[block] (next) {Next device};
        \node[decision, node distance=3cm, below of=next] (has) {Has event?};
        \node[block, node distance=3cm, below of=has] (get) {Get event};
        \node[block, below of=get] (process) {Dispatch event};

        \path[line] (next) -- (has);
        \path[line] (has.west) -- node[above] {No} ++(-1cm,0) |- (next.west);
        \path[line] (has) -- node {Yes} (get);
        \path[line] (get) -- (process);
        \path[line] (process.east) -- ++(1cm,0) |- (next.east);
    \end{tikzpicture}

    \caption[An illustration of the serial dispatcher.]{An illustration of the
    serial dispatcher. When "Next device" has reached the end of the list it
    restarts from the beginning again, thus the loop never breaks. Each node in
    this model blocks the rest of the execution. For instance, getting an event
    means sending a request over the network to the device which in turn
    responds with the latest event. The whole process is then blocked during
    the time it takes for the message to travel across the network.}

    \label{fig:dispatcher-serial}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=2.5cm, auto]
        \node[decision] (has) {Has event?};
        \node[block, node distance=3cm, below of=has] (get) {Get event};
        \node[block, below of=get] (process) {Dispatch event};

        \path[line] (has.west) -- node[above] {No} ++(-1cm,0) |- ++(0,2cm) -| (has.north);
        \path[line] (has) -- node {Yes} (get);
        \path[line] (get) -- (process);
        \path[line] (process.east) -- ++(1cm,0) |- (has.east);
    \end{tikzpicture}

    \caption[An illustration of a preemptive and cooperative dispatcher.]{An
    illustration of a preemptive and cooperative dispatcher. Each device can
    have their own dispatch process, so there is no need to loop through the
    list of device references.}

    \label{fig:dispatcher-pre-coop}
\end{figure}

What differentiates the cooperative dispatcher from the preemptive, though, is
that the dispatcher interleaves between devices implicitly and explicitly
respectively. The preemptive dispatcher can be implemented by creating one
thread per device and simply run a serial dispatcher for each thread that only
dispatch events from one device. The concurrency mechanism in the operating
system scheduler will perform the interleaving, thus making it implicit for the
user. The cooperative approach can be implemented with libuv, which has been
done in this study. By creating one state machine (like the one in Figure
\ref{fig:dispatcher-pre-coop}) per device and by letting the edges represent
asynchronous network transmissions, other device states can do work while the
network transmission is on the line. The explicit interleaving is happening
when the user initiates an asynchronous network transmission. The cooperative
approach only requires one thread, utilizing the asynchronous features of the
I/O drivers to the operating system.

Regarding resource usage in the different dispatcher models. The serial
approach will only communicate with one device at a time, thus only one socket
is required to be open concurrently. In resource usage, this can scale very
well.  If the list of device references becomes too large for memory, it can be
stored in a database. The preemptive and cooperative approaches will in theory
communicate with all devices concurrently, so there must be one socket
available for each device concurrently. This does not scale well since only a
limited amount of sockets are allowed by the operating system to be open. This
can however be solved by breaking down the devices into chunks that are
manageable by the gateway and process one chunk at a time.

\subsection{The event handler}

The purpose of the event handler is to process the event $\epsilon$. What type
of processing to be done depends on the application and what the event
represents. Returning to the weather station analogy; one application for the
events received from the temperature sensor could be to monitor the change in
temperature over time. This requires each event to be stored in some sort of
persistent data storage, e.g. a database. This database could either be found
locally on the gateway's filesystem or on some remote internet service. Perhaps
the weather station has a feature that predicts the future wind velocity in
real time. This requires some statistics or machine learning calculation to be
done every time a new event arrives at the gateway. Generalising this idea, we
get two different types of calculations: one depending on the CPU and its
memory, and one depending on some peripheral unit of the CPU, for instance the
file system or the network unit. The first we call CPU-work, or
$\lambda^{cpu}_j$, for it is work entirely performed by the CPU. The latter we
call I/O-work, or $\lambda^{io}_j$, for it is work not necessarily done by the
CPU but by some peripheral I/O unit. The definition then follows that the total
work done by the event handler $\Lambda_j = \lambda^{cpu}_j + \lambda^{io}_j$
for an event $\epsilon_j$. For simplification, it is assumed that work
$\Lambda_j$ is the same for each event. The $j$ subscript is removed and for
consistency with the rest of the paper the CPU- and I/O work are denoted
$\lambda_0$ and $\lambda_1$, respectively. Worth noting is that it is not
necessarily always the case that $\lambda_0$ is disjunct from $\lambda_1$, they
might depend on each other to some extent.

The serial event handler will perform $\lambda_0$ and $\lambda_1$
synchronously, one after another, see Figure \ref{fig:event_handler_serial}.
The entire work will block the rest of the execution.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[auto, >=stealth']
        \node[] (l0) {$\lambda_0$};
        \node[right = of l0] (l1) {$\lambda_1$};
        \node[right = of l1] (done) {Done};

        \draw[->] (l0) -- (l1);
        \draw[->] (l1) -- (done);
    \end{tikzpicture}

    \caption{Workflow of the serial event handler.}
    \label{fig:event_handler_serial}
\end{figure}

The preemptive event handler will conduct serial work on multiple threads, one
thread per event, see Figure \ref{fig:event_handler_preemptive}. The operating
system scheduler will perform what the user perceives as implicit interleaving.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[auto, >=stealth']
        \node[] (t0) {$Thread_0$};
        \node[right = of t0] (l0) {$\lambda_0$};
        \node[right = of l0] (l1) {$\lambda_1$};
        \node[right = of l1] (done) {Done};

        \draw[dashed] (t0) -- (l0);
        \draw[->] (l0) -- (l1);
        \draw[->] (l1) -- (done);

        \node[node distance = 0.5cm, below = of t0] (t1) {$Thread_1$};
        \node[right = of t1] (l0_2) {$\lambda_0$};
        \node[right = of l0_2] (l1_2) {$\lambda_1$};
        \node[right = of l1_2] (done_2) {Done};

        \draw[dashed] (t1) -- (l0_2);
        \draw[->] (l0_2) -- (l1_2);
        \draw[->] (l1_2) -- (done_2);

        \node[node distance = 0.5cm, below = of t1] (dots) {...};
    \end{tikzpicture}

    \caption[Workflow of the preemptive event handler.]{Workflow of the
    preemptive event handler. Serial work is done on multiple threads.}

    \label{fig:event_handler_preemptive}
\end{figure}

The cooperative event handler will perform CPU work on a single thread, see
Figure \ref{fig:event_handler_cooperative}.  However, I/O work will be
dispatched asynchronously to the associated I/O driver. Work can in that way be
parallelized with only one working thread.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[auto, >=stealth']
        \node[] (l0) {$\lambda_0$};
        \node[above right = of l0] (l1) {$\lambda_1$};
        \node[node distance = 1.5cm, right = of l0] (done) {Done};

        \draw[->, dashed] (l0) -- (l1);
        \draw[->] (l0) -- (done);

    \end{tikzpicture}

    \caption[Workflow of the cooperative event handler.]{Workflow of the
    cooperative event handler. CPU work $\lambda_0$ is performed on a single
    thread, I/O work $\lambda_1$ is however performed asynchronously, thus
    letting event handler return immediately after $\lambda_0$ is done.}

    \label{fig:event_handler_cooperative}
\end{figure}

\section{An abstract gateway}

A model of an abstract gateway is proposed as such: a gateway $\Gamma$ is a
six-tuple describing its internal properties and its environment.

\begin{equation}
\Gamma = \big \langle \Omega, \phi, \delta, K, \Lambda, \chi \big \rangle
\end{equation}

Let $\Omega$, $\phi$ and $\delta$ describe the properties of the environment of
$\Gamma$, also called the \textit{configuration} of $\Gamma$. The following
parameters are listed in Table \ref{tab:gateway_config} for easier reading.
$\Omega$ describes the devices communicating with $\Gamma$ and can be expressed
as a set $\Omega = \{ \omega_0, \omega_1, ..., \omega_{q-1} \}$ where
$\omega_i$ represents a device and $q$ is the total number of devices. Let
$\phi$ be the number of events $\omega_i$ generates each second and let
$\delta$ be the time it takes for a network request to travel from $\omega_i$
to $\Gamma$, or vice versa. $K$ and $\Lambda$ describes the internal properties
of $\Gamma$. Let $K$ describe the event propagation model of $\Gamma$. It is
expressed as $K = \{ \kappa_0, \kappa_1 \}$ where $\kappa_0$ and $\kappa_1$
represents the event propagation model of the dispatcher and the event handler
respectively and $\kappa_i \in \{ \textit{serial}, \textit{preemptive},
\textit{cooperative} \}$. Let $\Lambda$ describe the resource demand in
$\Gamma$ from each event. It is expressed as $\Lambda = \{ \lambda_0, \lambda_1
\}$ where $\lambda_0$ and $\lambda_1$ represents the CPU intensity and the I/O
intensity induced by each event, where $0 \leq \lambda_i \leq 1$. Let $\chi$
describe the hardware platform of $\Gamma$, specifically the number of cores on
the CPU.

\begin{table}[h!]
    \caption{An overview of the internal and external properties of $\Gamma$.}
    \label{tab:gateway_config}

    \begin{center}
        \begin{tabular}{|l|l|}
            \hline
            Parameter   & Description \\
            \hline
            $q$         & Number of devices. \\
            $\phi$      & Frequency of generated events. \\
            $\delta$    & The delay, or latency, added to the network. \\
            $\kappa_0$  & Event propagation model of the dispatcher. \\
            $\kappa_1$  & Event propagation model of the event handler. \\
            $\lambda_0$ & CPU intensity induced by each event. \\
            $\lambda_1$ & I/O intensity induced by each event. \\
            $\chi$      & Number of CPU cores. \\
            \hline
        \end{tabular}
    \end{center}
\end{table}

In addition to the rather static expression of an abstract gateway $\Gamma$,
the function $E$ is provided:

\begin{equation}
E(\Gamma, t_{start}, t_{end}) \to M = \big[ \epsilon_0, \epsilon_1, ...,
\epsilon_{m-1} \big]^T
\end{equation}

By providing $E$ with $\Gamma$ and two points in time $t_{start}$ and $t_{end}$
where $t_{start} \leq t_{end}$, a vector $M$ of ordered events $\epsilon_j$ is
returned, where $\epsilon_j = [ t_0^j, t_1^j, t_2^j, t_3^j, t_4^j ]$. Note that
$M$ is ordered by $t_0^j$ and $m$ is the total number of events. Each
$\epsilon_j$ include five timestamps $t_k^j$ that represents parts of the event
lifecycle:

\begin{description}
\item[$t_0^j$] represents the point in time when $\epsilon_j$ was created in
    $\omega_i$.
\item[$t_1^j$] represents the point in time when $\epsilon_j$ left $\omega_i$.
\item[$t_2^j$] represents the point in time when $\epsilon_j$ arrived at
    $\Gamma$.
\item[$t_3^j$] represents the point in time when $\epsilon_j$ was dispatched to
    the event handler in $\Gamma$.
\item[$t_4^j$] represents the point in time when $\Gamma$ finished processing
    $\epsilon_j$.
\end{description}

These timestamps holds a set of basic constraints:

\begin{equation}
t_{start} \leq t_k^j
\end{equation}

\begin{equation} \label{eq:t_0}
t_0^j \leq t_{end}
\end{equation}

\begin{equation}
t_0^j \leq t_1^j \leq t_2^j \leq t_3^j \leq t_4^j
\end{equation}

Let $\Gamma_c$ be a specific gateway with the configuration $c$.  Running
$E(\Gamma_c, t_{start}, t_{end})$ returns the vector $M_c$, which contains the
events produced by $\Gamma_c$.

\subsection{Performance metrics}
\label{sec:performance_metrics}

With $M$ and $\epsilon_j$ it is possible to derive some performance metrics.
Throughput $T$ can be calculated as:

$$
T = \frac{\sum_{\epsilon_j \in M}{\pi(\epsilon_j)}}{t_{end} - t_{start}}
\text{ [events / s]}
$$

It is also possible to analyze the response time for an event, how long time it
spent in $\omega_i$ and how long time it spent in $\Gamma$. Let:

\begin{description}

    \item[$d_0^j = t_4^j - t_0^j$] be the response time of $\epsilon_j$, i.e. the
        total time between that the event was created until it was processed.

    \item[$d_1^j = t_1^j - t_0^j$] be the wait time, i.e. the time $\epsilon_j$
        spent in $\omega_i$.

    \item[$d_2^j = t_4^j - t_2^j$] be the process time, i.e. $\epsilon_j$ spent
        in $\Gamma$.

\end{description}

Given a vector of events $M_c$, with length $m$, created by running $E$ on a
gateway with the configuration $c$, let $D_k^c$ be a vector containing $\big[
    d_k^0, d_k^1, ..., d_k^{m-1} \big]$, where $k = 0, 1, 2$, $d_0^j$ is the
response time, $d_1^j$ is the wait time and $d_2^j$ is the process time for
$\epsilon_j \in M_c$. Then:

\begin{description}

    \item[$avg(D_k^c)$] is the average value of all $d_k^j \in D_k^c$.

\end{description}

\subsection{Analysing the load}
\label{sec:load}

A system is said to be \textit{at load} when the system is working at full
capacity and is not able to produce higher throughput. Due to the
strict time limitation provided by $t_{end}$ there might be cases where some
events $\epsilon_j$ has $t_k^j > t_{end}$. These are events that haven't
been entirely processed by $\Gamma$. Let $\pi$ be a function such that:

\[
    \pi(\epsilon_j) =
\begin{cases}
    1, & \text{if } t_4^j \leq t_{end} \\
    0, & \text{otherwise}
\end{cases}
\]

With $\pi$, the load on $\Gamma$ can be analysed. The ratio between created
events and processed events can give hint on how many events $\Gamma$ can
handle. Let $P$ denote the ratio between created and processed events:

$$
P = \frac{P_\pi}{m} = \frac{\sum_{\epsilon_j \in M}{\pi(\epsilon_j)}}{m}
$$

$P$ will always be in the interval $0 \leq P \leq 1$, because $t_4^j$ can be
greater than $t_{end}$, which means $\pi(\epsilon_j) = 0$, thus $m \geq P_\pi$.
In cases where $P \simeq 1$, $\Gamma$ is able to process most of the events and
it might even be so that $\Gamma$ is waiting for new events to be created. In
that case, we can not be sure whether $\Gamma$ is at a proper load or not.
However, in cases where $P \ll 1$ we know for sure $\Gamma$ is not able to
process all events, so there must be events on the device waiting for $\Gamma$.
The response time, $t_4^j - t_0^j$, must therefore grow for each new event that
is created, leading to an unfair image of the response time. A proper load is
therefore one where $P \simeq 1$, but where an increase in total amount of
events $m$ only reduces $P$.

\section{Approach}

The goal of this thesis is to map how different gateway architectures affect
its performance in different environments. Using the abstract gateway model
$\Gamma$, it is possible to derive the performance mapping. By running $E$ with
a set of different gateways $\Gamma_c$ and analyse the throughput, load,
response time, wait time and process time on the resulting vector $M_c$ the
performance mapping will be developed. The abstract gateway will be implemented
to support a wide range of its possible configurations. In theory, the abstract
gateway allows different combinations of dispatcher and event handler event
propagation models. Due to lack of time, not all of them will be supported by
the implementation, see Table \ref{tab:event_prop_combinations}.

\begin{table}[h!]

    \caption[Overview of the combination of event propagation models.]{Overview
    of the combination of event propagation models available for the dispatcher
    and event handler.}

    \label{tab:event_prop_combinations}

    \begin{center}
        \begin{tabular}{|l|l|l|l|}
            \hline
            Dispatcher  & Event handler & Comment \\
            \hline
            Serial      & Serial        & Implemented. \\
            Serial      & Preemptive    & Implemented. \\
            Serial      & Cooperative   & Not implemented. \\
            Preemptive  & Serial        & Not implemented. \\
            Preemptive  & Preemptive    & Not implemented. \\
            Preemptive  & Cooperative   & Not implemented. \\
            Cooperative & Serial        & Not implemented. \\
            Cooperative & Preemptive    & Implemented. \\
            Cooperative & Cooperative   & Implemented. \\
            \hline
        \end{tabular}
    \end{center}
\end{table}

To mimick the gateway hardware used by Customer A (see Section
\ref{sec:customer-cases}) a Raspberry Pi 3 Model
B\footnote{\url{https://www.raspberrypi.org/products/raspberry-pi-3-model-b/}}
is used to run the gateway implementation. The system that simulates the
devices and analyses the resulting events (later known as the \textit{test
manager}, see Chapter \ref{ch:implementation}) is run on a Macbook
Pro\footnote{\url{https://support.apple.com/kb/sp703?locale=sv_SE}}. The
hardware used are described in Table \ref{tab:test_environment_hardware}.

\begin{table}[h!]
    \caption{Overview of the hardware used in the performance tests.}
    \label{tab:test_environment_hardware}

    \begin{center}
        \begin{tabular}{|l|l|p{4cm}|l|}
            \hline
            Usage           & Model                     & CPU  & Memory size \\
            \hline
            Gateway         & Raspberry Pi 3 Model B    & Quad Core 1.2GHz Broadcom
            BCM2837 64bit  & 1 GB \\

            Test manager    & Macbook Pro               & Dual Core 2.6Ghz
            Intel Core i5    & 8 GB \\
            \hline
        \end{tabular}
    \end{center}
\end{table}

The configuration parameter $\chi$ allows the number of cores to change. This
is implemented by setting a flag in the operating system running the Raspberry
Pi that sets the maximum number of cores used. Only wireless LAN is used
between the test manager and the gateway.
