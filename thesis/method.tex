\chapter{Method}
\label{cha:method}

The goal of this study is to map out and understand how IoT gateway
architectures can be developed and how they perform. Also, the asynchronous I/O
library libuv will be studied to see how it can increase the performance of IoT
gateways.

\section{Three phases}

The overall phases of the study has been a design phase, a technological phase
and a reflexive phase \cite{novikov2013research}. In the design phase, the
concept and goals of the study were developed. Together with the author's own
ideas and input from Attentec, the value of understanding IoT gateway
performance was identified. The hypothesis that the libuv library will improve
IoT gateway performance was stated and a theoretical framework was mapped out
as previous research was studied.

The theoretical framework was developed in this phase to support claims and
form a general direction of the entire study. Multiple databases were queried
in order to find interesting material from previous research. Mainly the online
library hosted by \textit{Linköping University}\footnote{\url{www.bibl.liu.se}}
was used since it allow access to material otherwise unviewable due to
institutional login requirements. Query results from this library is a
collection of query results from other research databases such as \textit{ACM
Digital Library}\footnote{\url{dl.acm.org}}, \textit{ProQuest Ebook
Central}\footnote{\url{ebookcentral.proquest.com}} and \textit{IEEE Xplore
Digital Library}\footnote{\url{ieeexplore.ieee.org}}; so it acts as a gateway
to a global collection of scientific research.

The \textit{three-pass approach} presented by Keshav \cite{keshav2007read} was
be used as a basic approach to find interesting material. It helps the reader
grasp the paper's content in three \textit{passes}. The first pass' purpose is
to give the reader an overview of the paper. The title, abstract, introduction,
headings, sub-headings, conclusions and references are read. This information
should help the reader understand the paper's category and context and help
decide whether to continue read this paper or leave it. If the reader choose to
continue read it, the second pass starts. Here the paper is read more
thoroughly. The figures and diagrams are examined and after this pass the
reader should be able to summarize the paper, with leading evidence, to someone
else. The purpose of the third pass is to fully understand the paper. By making
the same assumptions as the author, the paper is virtually re-created. It helps
identify the true innovations of the paper, as well as the hidden failures.

Together with Attentec, a model of a general, or abstract, gateway was
theoretically developed. The idea was that all IoT gateway implementations have
some common features, constraints and environments that can be simulated and
tested. Another hypothesis developed that different IoT gateway architectures
performed different in different environments, and that there is no
"one-size-fit-all" architecture.

The next phase, the technological phase, started with implementation of the
abstract gateway in order to enable empirical data collection of different
performance measures of different configurations of the abstract gateway. Once
the implementation was considered done, the data collection began.

In the reflexive phase the empirical data was interpreted. There were efforts
in filtering the data and prioritizing what was the most interesting and
significant data that would lead to the best result. The results were then
formulated in this thesis.

\section{Related work}

Previous attempts have been made to prove how certain programming languages
perform better when used in a reactive context. Terber
\cite{terber2017function} discusses the lack of function-oriented software
decomposition for reactive software. With an industrial application as context,
he replaces legacy code with code written in the \textit{Cèu} programming
language, reaching the conclusion that Cèu preserves fundamental software
engineering principles and is at the same time able to fullfill resource
limitations in the system. Jagadeesan et al \cite{jagadeesan1996formal}
performs a similar study but with a different language: \textit{Esterel}. They
reimplement a component in a telephone switching system and reach the
conclusion that Esterel is better suited for analysis and verification for
reactive systems.

No previous research has been found regarding the internal architecture of the
IoT gateway and how it affects performance. Most research regarding IoT
gateways discuss architecture on an application level. Performance testing is
also to a certain degree a non-explored area and most of the literature takes
on empirical approaches, especially for distributed systems performance.

Chen et al. \cite{chen2011brief} describes the IoT gateway as the bridge
between the sensing domain in the IoT architecture and the network domain and
argues its importance as one of the most significant in the IoT architecture.
They describe some common features of the gateway such as support for multiple
network interfaces (2G/3G, LTE, LAN), protocol conversion and manageability. A
reference model is presented where these features are taken into account.
However, the paper is not focusing on the same architectural abstraction level
as this thesis, but provides a more holistic view on the entire IoT gateway
application and sub-applications.

Zachariah et al. \cite{zachariah2015internet} argues that application specific
IoT gateways are not the best approach towards a scalable IoT infrastructure.
The same reason there isn't one web browser per web page, gateways should be
generic and support all types of IoT devices and sensors. They propose a
smartphone-centric architecture where IoT devices connect to the internet via
smartphones. This paper is valuable because it challenges the traditional mind
around IoT gateways. It is however elaborating the gateway on a higher
abstraction level than this thesis is.

Weyuker et al. \cite{weyuker2000experience} states there has been very little
research on software performance testing. In their paper, they discuss
performance testing objectives, workload design and the role of requirements
and specifications to eventually lead up to a case study where they test a
gateway system without having access to historical usage data. They develop
test cases based on usage scenarios to adhere their formulated performance
objectives. The tests are conducted by running programs that simulate clients
in the system, which sends transaction requests in various sizes and
frequencies. The rate of transactions are monitored and used to assess the
performance of the entire system. Their reflections and discussions on
performance testing at large are very valuable to this thesis. However, their
paper focuses on an already existing application upon which performance testing
will be used to assess whether it fulfils its functional and non-functional
requirements. This thesis dives into how an abstract gateway can be modeled and
implemented and how its different configurations affects performance for some
given contexts.

Denaro et al. \cite{denaro2005performance} presents a systematic method to
ascertain that a given distributed software architecture lives up its
performance requirements. They mention some performance metrics such as
latency, throughput and scalability. They state that the overall performance of
a distributed system is often determined by the middleware used, and that same
middleware can affect performance differently depending on its context of
application. This is a good incentive to map and understand IoT gateway
performance, which is done in this thesis.

Kruger et al. \cite{kruger2014benchmarking} discuss the common use of
off-the-shelf components for implementing IoT devices and gateways. They state
little work has previously been done to explore the performance of these
devices when different parameters are adjusted; such as system architecture and
software components. They focus primarily on hardware adjustments (e.g.
processor and memory speed) to identify performance bottlenecks in three common
off-the-shelf brands: Raspberry Pi, Beaglebone and Beaglebone Black. They use
\textit{micro-benchmarks} to identify performance bottlenecks at architecture
level and \textit{macro-benchmarks} at application level. They reach the
conclusion that gateway performance is most important when there is a large
amount of data being transfered between the gateway and the IoT application.

Aguilera et al. \cite{aguilera2003performance} presents an approach to identify
performance bottlenecks in distributed systems where the components are seen as
black boxes, i.e. sub-systems of unknown functionality. By monitoring the
traffic of \textit{remote procedure calls} in the system and store them in a
log they are able to find the \textit{casual paths} of messages and thereby
understand the data flow in the system and where performance bottlenecks are
located. The approach to capture log messages is used in this thesis, however
the step of storing messages in a log file before computation is omitted and
each log message is computed as it arrives.

Liu et al. \cite{liu2002designing} presents an approach to predict distributed
system performance empirically by running test suites. They map out some
parameters that affect performance, such as client request load and thread pool
size and some observable parameters such as throughout and response time. They
reach the conclusion that gathering empirical results with simple test cases is
an effective method to understand the performance profile of the system.

\section{Two customer cases}
\label{sec:customer-cases}

Before presenting the architectural models used in this study, it is of
importance to understand how they came about. Among the customers of Attentec,
two were of interest for this study with each having implemented an IoT gateway
suitable for its operating context. \textit{Customer A} uses their IoT gateway
to monitor industrial batteries used in forklifts. The main idea behind the
event propagation model is that the gateway polls each battery for events and
process them according to its specification (which is unrelevant here).
Referring to data propagation in reactive languages (see section
\ref{sec:evaluation-model}), one can view the event propagation as
\textit{pull-based}, i.e. the gateway pulls events from the batteries whenever
it finds it necessary. The batteries are \textit{passive}, i.e. they do not
emit any events on their own, only when asked to by the gateway. Figure
\ref{fig:pull-based} illustrates this.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[auto]
        \node [block] (device_1) {Device 1};
        \node [block, right of=device_1, node distance=3cm] (device_2) {Device 2};
        \node [block, right of=device_2, node distance=3cm] (device_n) {Device n};
        \node [block, below of=device_2, node distance=2.5cm] (gateway) {Gateway};

        \path [->] (device_1) edge [bend left=5] (gateway);
        \path [->] (gateway) edge [bend left=5] (device_1);
        \path [->] (device_2) edge [bend left=5] (gateway);
        \path [->] (gateway) edge [bend left=5] (device_2);
        \path [->] (device_n) edge [bend left=5] (gateway);
        \path [->] (gateway) edge [bend left=5] (device_n);
    \end{tikzpicture}
    \caption[Pull-based event propagation]{Pull-based event propagation with
    passive devices. The gateway pulls events from the devices by requesting
    them first.}
    \label{fig:pull-based}
\end{figure}

\textit{Customer B} on the other hand uses a \textit{push-based} approach to
handle event propagation in their gateway. The gateway hosts a REST API to
serve IoT devices. This means that the devices must be \textit{active}, i.e.
they emit events whenever they find it necessary. See Figure
\ref{fig:push-based} for an illustration. Note that not as many communication
requests are necessary in this approach compared to the pull-based one. The
gateway does not need to know about how many devices there are or what their
addresses are, but the devices need to know the address of the gateway. In
contrast, the pull-based approach requires the gateway to know about each
device, but the device does not need to know anything about the gateway.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[auto]
        \node [block] (device_1) {Device 1};
        \node [block, right of=device_1, node distance=3cm] (device_2) {Device 2};
        \node [block, right of=device_2, node distance=3cm] (device_n) {Device n};
        \node [block, below of=device_2, node distance=2.5cm] (gateway) {Gateway};

        \path [line] (device_1) -- (gateway);
        \path [line] (device_2) -- (gateway);
        \path [line] (device_n) -- (gateway);
    \end{tikzpicture}
    \caption[Push-based event propagation]{Push-based event propagation with
    active devices. Events are pushed from the devices to the gateway that
    listens for incoming requests.}
    \label{fig:push-based}
\end{figure}

\section{Three event propagation models}

Taking inspiration from the three task management approaches presented by Adya
et al \cite{adya2002cooperative} and described in Section
\ref{sec:task-management}, three event propagation models are proposed:
\textit{serial}, \textit{preemptive} and \textit{cooperative event
propagation}. Before presenting a formal definition, an analogy might suite
well here. Imagine a cafe that serves coffee. There are several customers in
line and the barista takes the order from the next customer in line, brews the
coffee and serves it. The coffee brewer can only brew one cup of coffee at a
time. This is repeated for each customer in line with the constraint that no
new order is accepted before the current order is served, see Figure
\ref{fig:cafe-serial}. This can be seen as a \textit{serial order handling};
only one order can be handled at a time. Imagine now that the cafe hires more
baristas and more coffee machines. There is still one line (but there can be
several), but orders can be handled concurrently, so the next customer in line
does not necessarily have to wait for the previous customer to be served before
he can place his order, see Figure \ref{fig:cafe-preemptive}. This can be seen
as \textit{preemptive order handling}. Now lets keep the brewers but let go all
baristas except for one.  All customers place their order to the same barista
which in turn start one of the brewers. While the brewer is working, the next
customer in line places his order and the barista can start a second brewer,
see Figure \ref{fig:cafe-cooperative}. This is called \textit{cooperative order
handling}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=5cm, auto, >=stealth']
        \node[] (brewer) {Brewer};
        \node[right of=brewer] (barista) {Barista};
        \node[right of=barista] (customers) {Customer line};

        \node[below of=brewer, node distance=7.5cm] (brewer_ground) {};
        \node[below of=barista, node distance=7.5cm] (barista_ground) {};
        \node[below of=customers, node distance=7.5cm] (customers_ground) {};

        \draw (brewer) -- (brewer_ground);
        \draw (barista) -- (barista_ground);
        \draw (customers) -- (customers_ground);

        \draw[<-] ($(barista)!1cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_0$}
            ($(customers)!1cm!(customers_ground)$);
        \draw[<-] ($(brewer)!1.1cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista)!1.1cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer)!1.5cm!(brewer_ground)$) {Brewing...};
        \draw[->] ($(brewer)!1.9cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista)!1.9cm!(barista_ground)$);
        \draw[->] ($(barista)!2cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_0$}
            ($(customers)!2cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!2.5cm!(customers_ground)$)
            {Next customer};

        \draw[<-] ($(barista)!3cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_1$}
            ($(customers)!3cm!(customers_ground)$);
        \draw[<-] ($(brewer)!3.1cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista)!3.1cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer)!3.5cm!(brewer_ground)$) {Brewing...};
        \draw[->] ($(brewer)!3.9cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista)!3.9cm!(barista_ground)$);
        \draw[->] ($(barista)!4cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_1$}
            ($(customers)!4cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!4.5cm!(customers_ground)$)
            {Next customer};

        \node[draw=none, fill=white, scale=0.9] at ($(brewer)!5cm!(brewer_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista)!5cm!(barista_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!5cm!(customers_ground)$)
            {...};

        \draw[<-] ($(barista)!6cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_j$}
            ($(customers)!6cm!(customers_ground)$);
        \draw[<-] ($(brewer)!6.1cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista)!6.1cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer)!6.5cm!(brewer_ground)$) {Brewing...};
        \draw[->] ($(brewer)!6.9cm!(brewer_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista)!6.9cm!(barista_ground)$);
        \draw[->] ($(barista)!7cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_j$}
            ($(customers)!7cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at ($(brewer)!7.5cm!(brewer_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista)!7.5cm!(barista_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!7.5cm!(customers_ground)$)
            {...};
    \end{tikzpicture}
    \caption{A sequence diagram of a serial cafe order line. Each customer is
    served one at a time and no customer is served before the previous customer
    receives its order.}
    \label{fig:cafe-serial}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=3cm, auto, >=stealth']
        \node[] (brewern) {$Brewer_{n-1}$};
        \node[right of=brewern] (baristan) {$Barista_{n-1}$};
        \node[right of=baristan, node distance=1.5cm] (dots) {...};
        \node[right of=dots, node distance=1.5cm] (brewer0) {$Brewer_0$};
        \node[right of=brewer0] (barista0) {$Barista_0$};
        \node[right of=barista0] (customers) {Customer line};

        \node[below of=brewern, node distance=7cm] (brewern_ground) {};
        \node[below of=baristan, node distance=7cm] (baristan_ground) {};
        \node[below of=dots, node distance=7cm] (dots_ground) {};
        \node[below of=brewer0, node distance=7cm] (brewer0_ground) {};
        \node[below of=barista0, node distance=7cm] (barista0_ground) {};
        \node[below of=customers, node distance=7cm] (customers_ground) {};

        \draw (brewern) -- (brewern_ground);
        \draw (baristan) -- (baristan_ground);
        \draw (brewer0) -- (brewer0_ground);
        \draw (barista0) -- (barista0_ground);
        \draw (customers) -- (customers_ground);

        \draw[<-] ($(barista0)!1cm!(barista0_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_0$}
            ($(customers)!1cm!(customers_ground)$);
        \draw[<-] ($(brewer0)!1.1cm!(brewer0_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista0)!1.1cm!(barista0_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer0)!1.5cm!(brewer0_ground)$) {Brewing...};
        \draw[->] ($(brewer0)!2.9cm!(brewer0_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista0)!2.9cm!(barista0_ground)$);
        \draw[->] ($(barista0)!3cm!(barista0_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_0$}
            ($(customers)!3cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!1.5cm!(customers_ground)$)
            {Next customer};

        \draw[<-, dashed] ($(dots)!2cm!(dots_ground)$) --
            node[above, scale=0.9, pos=0.4]{Place $Order_1$}
            ($(customers)!2cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!3.5cm!(customers_ground)$)
            {Next customer};

        \node[draw=none, fill=white, scale=0.9] at ($(brewern)!4cm!(brewern_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(baristan)!4cm!(baristan_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(brewer0)!4cm!(brewer0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista0)!4cm!(barista0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!4cm!(customers_ground)$)
            {...};

        \draw[<-] ($(baristan)!4.5cm!(baristan_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_j$}
            ($(customers)!4.5cm!(customers_ground)$);
        \draw[<-] ($(brewern)!4.6cm!(brewern_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(baristan)!4.6cm!(baristan_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewern)!5.5cm!(brewern_ground)$) {Brewing...};
        \draw[->] ($(brewern)!6.4cm!(brewern_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(baristan)!6.4cm!(baristan_ground)$);
        \draw[->] ($(baristan)!6.5cm!(baristan_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_j$}
            ($(customers)!6.5cm!(customers_ground)$);

        \draw[->, dashed] ($(dots)!5.5cm!(dots_ground)$) --
            node[above, scale=0.9, pos=0.41]{Serve $Order_{j-1}$}
            ($(customers)!5.5cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at ($(brewern)!7cm!(brewern_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(baristan)!7cm!(baristan_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(brewer0)!7cm!(brewer0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista0)!7cm!(barista0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!7cm!(customers_ground)$)
            {...};
    \end{tikzpicture}
    \caption{A sequence diagram of a preemptive cafe order line. There are
    $n$ baristas and brewers that can handle orders concurrently and
    customers does not necessarily have to wait for previous orders to be
    returned before they can place their own.}
    \label{fig:cafe-preemptive}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=3cm, auto, >=stealth']
        \node[] (brewern) {$Brewer_{n-1}$};
        \node[right of=brewern, node distance=1.5cm] (dots) {...};
        \node[right of=dots, node distance=1.5cm] (brewer0) {$Brewer_0$};
        \node[right of=brewer0] (barista) {Barista};
        \node[right of=barista] (customers) {Customer line};

        \node[below of=brewern, node distance=7cm] (brewern_ground) {};
        \node[below of=dots, node distance=7cm] (dots_ground) {};
        \node[below of=brewer0, node distance=7cm] (brewer0_ground) {};
        \node[below of=barista, node distance=7cm] (barista_ground) {};
        \node[below of=customers, node distance=7cm] (customers_ground) {};

        \draw (brewern) -- (brewern_ground);
        \draw (brewer0) -- (brewer0_ground);
        \draw (barista) -- (barista_ground);
        \draw (customers) -- (customers_ground);

        \draw[<-] ($(barista)!1cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_0$}
            ($(customers)!1cm!(customers_ground)$);
        \draw[<-] ($(brewer0)!1.1cm!(brewer0_ground)$) --
            node[above, scale=0.9, midway]{Start brewer}
            ($(barista)!1.1cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewer0)!1.5cm!(brewer0_ground)$) {Brewing...};
        \draw[->] ($(brewer0)!2.9cm!(brewer0_ground)$) --
            node[above, scale=0.9, midway]{Coffee ready}
            ($(barista)!2.9cm!(barista_ground)$);
        \draw[->] ($(barista)!3cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_0$}
            ($(customers)!3cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!1.5cm!(customers_ground)$)
            {Next customer};

        \draw[<-] ($(barista)!2cm!(barista_ground)$) --
            node[above, scale=0.9, pos=0.4]{Place $Order_1$}
            ($(customers)!2cm!(customers_ground)$);
        \draw[<-, dashed] ($(dots)!2.1cm!(dots_ground)$) --
            node[above, scale=0.9, pos=0.65]{Start brewer}
            ($(barista)!2.1cm!(barista_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(customers)!3.5cm!(customers_ground)$)
            {Next customer};

        \node[draw=none, fill=white, scale=0.9] at ($(brewern)!4cm!(brewern_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(brewer0)!4cm!(brewer0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista)!4cm!(barista_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!4cm!(customers_ground)$)
            {...};

        \draw[<-] ($(barista)!4.5cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Place $Order_j$}
            ($(customers)!4.5cm!(customers_ground)$);
        \draw[<-] ($(brewern)!4.6cm!(brewern_ground)$) --
            node[above, scale=0.9, pos=0.75]{Start brewer}
            ($(barista)!4.6cm!(barista_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(brewern)!5.5cm!(brewern_ground)$) {Brewing...};
        \draw[->] ($(brewern)!6.4cm!(brewern_ground)$) --
            node[above, scale=0.9, pos=0.75]{Coffee ready}
            ($(barista)!6.4cm!(barista_ground)$);
        \draw[->] ($(barista)!6.5cm!(barista_ground)$) --
            node[above, scale=0.9, midway]{Serve $Order_j$}
            ($(customers)!6.5cm!(customers_ground)$);

        \draw[->, dashed] ($(dots)!5.5cm!(dots_ground)$) --
            node[above, scale=0.9, pos=0.41]{Serve $Order_{j-1}$}
            ($(customers)!5.5cm!(customers_ground)$);

        \node[draw=none, fill=white, scale=0.9] at ($(brewern)!7cm!(brewern_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(brewer0)!7cm!(brewer0_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(barista)!7cm!(barista_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at ($(customers)!7cm!(customers_ground)$)
            {...};
    \end{tikzpicture}
    \caption{A sequence diagram of a cooperative cafe order line. There is only
    one barista but $n$ brewers. As soon as a brewer is working, the next order
    can be handled.}
    \label{fig:cafe-cooperative}
\end{figure}

Looking at all three of these order handling approaches, we find a few common
entities. They all have orders, a customer line, one or several baristas and
brewers. Lets generalize this a bit and call orders \textit{events}, the
customer line we call the \textit{dispatcher}, the barista and brewer combined
we call the \textit{event handler} and the work done by the dispatcher and the
event handler on event $\epsilon_j$ we call $\lambda^d_j$ and $\lambda^e_j$
respectively and the total work $\Lambda_j = \lambda^d_j + \lambda^e_j$. The
definition of the three event propagation models is as follows:

\todo{A better explaining on the work $\Lambda$}

\begin{description}

    \item[\textbf{Serial event propagation}:] Given a set of $m$ events $\{
            \epsilon_0, \epsilon_1, ..., \epsilon_j, ..., \epsilon_{m-1} \}$ no
        event $\epsilon_j$ is dispatched before the work $\Lambda_{j-1}$ is
        finished, see Figure \ref{fig:event-propagation-serial}.

    \item[\textbf{Preemptive event propagation}:] Given a set of $m$ events $\{
            \epsilon_0, \epsilon_1, ..., \epsilon_j, ..., \epsilon_{m-1} \}$
        each event $\epsilon_j$ can be dispatched independent on
        $\Lambda_{j-1}$ and the work $\Lambda_j$ can implicitly interleave and
        make room for work $\Lambda_k$ where $k \ne j$.

    \item[\textbf{Cooperative event propagation}:] Given a set of $m$ events
        $\{ \epsilon_0, \epsilon_1, ..., \epsilon_j, ..., \epsilon_{m-1} \}$
        each event $\epsilon_j$ can be dispatched independent on
        $\Lambda_{j-1}$ and the work $\Lambda_j$ can explicitly make room for
        work $\Lambda_k$ where $k \ne j$.

\end{description}

Common for all models is that the event $\epsilon_j$ can not be dispatched
before the work $\lambda^d_j$ is done and the work $\lambda^e_j$ can not start
before the event $\epsilon_j$ is dispatched. Each event follow the order
$\epsilon_j \rightarrow \lambda^d_j \rightarrow dispatch \rightarrow
\lambda^e_j \rightarrow finish$.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=5cm, auto, >=stealth']
        \node[] (event_handler) {Event handler};
        \node[right of=event_handler] (dispatcher) {Dispatcher};

        \node[below of=event_handler, node distance=8cm] (event_handler_ground) {};
        \node[below of=dispatcher, node distance=8cm] (dispatcher_ground) {};

        \draw (event_handler) -- (event_handler_ground);
        \draw (dispatcher) -- (dispatcher_ground);

        \node[draw=none, fill=white, scale=0.9] at
            ($(dispatcher)!0.5cm!(dispatcher_ground)$)
            {$\lambda^d_0$};

        \draw[<-] ($(event_handler)!1cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Dispatch $\epsilon_0$}
            ($(dispatcher)!1cm!(dispatcher_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(event_handler)!1.5cm!(event_handler_ground)$)
            {$\lambda^e_0$};
        \draw[->] ($(event_handler)!2cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Finish $\epsilon_0$}
            ($(dispatcher)!2cm!(dispatcher_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(dispatcher)!2.5cm!(dispatcher_ground)$)
            {$\lambda^d_1$};

        \draw[<-] ($(event_handler)!3cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Dispatch $\epsilon_1$}
            ($(dispatcher)!3cm!(dispatcher_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(event_handler)!3.5cm!(event_handler_ground)$)
            {$\lambda^e_1$};
        \draw[->] ($(event_handler)!4cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Finish $\epsilon_1$}
            ($(dispatcher)!4cm!(dispatcher_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(dispatcher)!4.5cm!(dispatcher_ground)$)
            {$\lambda^d_2$};

        \node[draw=none, fill=white, scale=0.9] at
        ($(event_handler)!5cm!(event_handler_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at
        ($(dispatcher)!5cm!(dispatcher_ground)$)
            {...};

        \draw[->] ($(event_handler)!5.5cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Finish $\epsilon_{j-1}$}
            ($(dispatcher)!5.5cm!(dispatcher_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
            ($(dispatcher)!6cm!(dispatcher_ground)$)
            {$\lambda^d_j$};

        \draw[<-] ($(event_handler)!6.5cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Dispatch $\epsilon_j$}
            ($(dispatcher)!6.5cm!(dispatcher_ground)$);
        \node[draw=none, fill=white, scale=0.9] at
            ($(event_handler)!7cm!(event_handler_ground)$)
            {$\lambda^e_j$};
        \draw[->] ($(event_handler)!7.5cm!(event_handler_ground)$) --
            node[above, scale=0.9, midway]{Finish $\epsilon_j$}
            ($(dispatcher)!7.5cm!(dispatcher_ground)$);

        \node[draw=none, fill=white, scale=0.9] at
        ($(event_handler)!8cm!(event_handler_ground)$)
            {...};
        \node[draw=none, fill=white, scale=0.9] at
        ($(dispatcher)!8cm!(dispatcher_ground)$)
            {...};
    \end{tikzpicture}
    \caption{The serial event propagation model. Only one event $\epsilon_j$ is
    dispatched and handled at a time and no event $\epsilon_j$ can be
    dispatched before the previous work $\Lambda_{j-1}$ is finished.}
    \label{fig:event-propagation-serial}
\end{figure}

\subsection{The anatomy of an event}

The event is the actual data transmitted from the device to the gateway.
Imagine a small weather station used at private homes. It has two sensors
measuring wind velocity and air temperature. The gateway is the central system
passing the data from each sensor to the application using the data. At a
certain frequency, the gateway polls each sensor for new velocity and
temperature values. These values are the actual events in the system. In this
toy example, each event might not only carry the actual value of the sensor it
corresponds to, but also information on what sensor this data is coming from.
The weather application might draw a plot on how the air temperate has changed
over time, therefore each event must also include the timestamp of each sensor
value. In this study, however, there is no need for event data that represents
some mimicked sensor value. What is needed is a way to track each event to see
when it reaches certain places in its life. This is done by registering the
timestamp when the event reaches those places, see Figure \ref{fig:event-life}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=3cm, auto]
        \node[block] (device) {Device};
        \node[below of=device] (inv) {};
        \node[block, below of=inv] (dispatcher) {Dispatcher};
        \node[block, below of=dispatcher] (event_handler) {Event Handler};

        \path[line] (device) -- (dispatcher);
        \path[line] (dispatcher) -- (event_handler);

        \node[comment, node distance=1cm, right =of device] (created) {Created};
        \node[comment, align=left, node distance=1cm, right =of inv] (fetched_retrieved)
            {Leaves device /\\ arrives at gateway};
        \node[comment, node distance=1cm, right =of dispatcher] (dispatched) {Dispatched};
        \node[comment, node distance=1cm, right =of event_handler] (processed) {Processed};
    \end{tikzpicture}
    \caption{The event travels from the device down to the event handler. At
    each step the time point is registered, shown in the clouds.}
    \label{fig:event-life}
\end{figure}

An event $\epsilon$ can generally be considered as a vector

$$
\epsilon = \big[ t_0, t_1, ..., t_{k-1} \big]
$$

where $t_i$ represents the timestamp when $\epsilon$ reached place $i$ in its
life. Let $k$ be the number of different places the event can reach. In this
study, five places are registered: the creation of the event, when the event
leaves the device, when the event reaches the gateway, when the event is
dispatched to the event handler and when the event is processed by the event
handler.

\subsection{The dispatcher}

The purpose of the dispatcher is to listen to or poll for events and then
dispatch them to the event handler. As mentioned in Section
\ref{sec:customer-cases}, there are two main approaches to this: pull-based or
push-based. The devices generating the events are either active and can push
each event to the gateway, which in turn has to listen to them, or the devices
are passive and the gateway must pull events from them. In this study, the
pull-based approach is used to mimick the gateway used by Customer A. This
approach requires the dispatcher to have a reference to each device to be able
to pull events from them.

An important question to consider is: how is the dispatcher related to the
event propagation models? If the architecture of the gateway implements the
serial event propagation model, the dispatcher should make sure it does not
dispatch an event before the previous event is processed, see Figure
\ref{fig:dispatcher-serial}. Both the preemptive and the cooperative event
propagation models loosens this constraint and allows the dispatcher to
dispatch events before previous events have been processed. In contrast to the
serial approach in Figure \ref{fig:dispatcher-serial}, all devices can be
polled for events concurrently. There is no need, in theory, to loop through
the list of device references and check them one by one. Instead, there can be
one dispatch process for each device, see Figure \ref{fig:dispatcher-pre-coop}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=2.5cm, auto]
        \node[block] (next) {Next device};
        \node[decision, node distance=3cm, below of=next] (has) {Has event?};
        \node[block, node distance=3cm, below of=has] (get) {Get event};
        \node[block, below of=get] (process) {Process event};

        \path[line] (next) -- (has);
        \path[line] (has.west) -- node[above] {No} ++(-1cm,0) |- (next.west);
        \path[line] (has) -- node {Yes} (get);
        \path[line] (get) -- (process);
        \path[line] (process.east) -- ++(1cm,0) |- (next.east);
    \end{tikzpicture}

    \caption{An illustration of the serial dispatcher. When "Next device" has
    reached the end of the list it restarts from the beginning again, thus the
    loop never breaks. Each node in this model blocks the rest of the
    execution. For instance, getting an event means sending a request over the
    network to the device which in turn responds with the latest event. The
    whole process is then blocked during the time it takes for the message to
    travel across the network.}

    \label{fig:dispatcher-serial}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=2.5cm, auto]
        \node[decision] (has) {Has event?};
        \node[block, node distance=3cm, below of=has] (get) {Get event};
        \node[block, below of=get] (process) {Process event};

        \path[line] (has.west) -- node[above] {No} ++(-1cm,0) |- ++(0,2cm) -| (has.north);
        \path[line] (has) -- node {Yes} (get);
        \path[line] (get) -- (process);
        \path[line] (process.east) -- ++(1cm,0) |- (has.east);
    \end{tikzpicture}

    \caption{An illustration of a preemptive and cooperative dispatcher. Each
    device can have their own dispatch process, so there is no need to loop
    through the list of device references.}

    \label{fig:dispatcher-pre-coop}
\end{figure}

What differentiates the cooperative dispatcher from the preemptive, though, is
that the dispatcher interleaves between devices implicitly and explicitly
respectively. The preemptive dispatcher can be implemented by creating one
thread per device and simply run a serial dispatcher for each thread that only
dispatch events from one device. The concurrency mechanism in the operating
system scheduler will perform the interleaving, thus making it implicit for the
user. The cooperative approach can be implemented with libuv, which has been
done in this study. By creating one state machine (like the one in Figure
\ref{fig:dispatcher-pre-coop}) per device and by letting the edges represent
asynchronous network transmissions, other device states can do work while the
network transmission is on the line. The explicit interleaving is happening
when the user initiates an asynchronous network transmission. The cooperative
approach only requires one thread, utilizing the asynchronous features of the
I/O drivers to the operating system.

\subsection{The event handler}

The purpose of the event handler is to process the event $\epsilon$. What type
of processing to be done depends on the application and what the event
represents. Returning to the weather station analogy; one application for the
events received from the temperature sensor could be to monitor the change in
temperature over time. This requires each event to be stored in some sort of
persistent data storage, e.g. a database. This database could either be found
locally on the gateway's filesystem or on some remote internet service. Perhaps
the weather station has a feature that predicts the future wind velocity in
real time. This requires some statistics or machine learning calculation to be
done every time a new event arrives at the gateway. Generalising this idea, we
get two different types of calculations: one depending on the CPU and its
memory, and one depending on some peripheral unit of the CPU, for instance the
file system or the network unit. The first we call CPU-work, or $\lambda^0$,
for it is work entirely performed by the CPU. The latter we call I/O-work, or
$\lambda^1$, for it is work not necessarily done by the CPU but by some
peripheral I/O unit. The definition then follows that the total work done by
the event handler $\lambda^e_j = \lambda^0_j + \lambda^1_j$ for an event
$\epsilon_j$. It is however not necessarily the case that $\lambda^0$ is
disjunct from $\lambda^0$, they might depend on each other to some extent.

\section{An abstract gateway}

A model of an abstract gateway is proposed as such: a gateway $\Gamma$ is a
six-tuple describing its internal properties and its environment.

\begin{equation}
\Gamma = \big \langle \Omega, \phi, \delta, K, \Lambda, \chi \big \rangle
\end{equation}

\todo{active/passive devices?}

Let $\Omega$, $\phi$ and $\delta$ describe the properties of the environment of
$\Gamma$, also called the \textit{configuration} of $\Gamma$. The following
parameters are listed in Table \ref{tab:gateway_config} for easier reading.
$\Omega$ describes the devices communicating with $\Gamma$ and can be expressed
as a set $\Omega = \{ \omega_0, \omega_1, ..., \omega_{q-1} \}$ where
$\omega_i$ represents a device and $q$ is the total number of devices. Let
$\phi$ be the number of events $\omega_i$ generates each second and let
$\delta$ be the time it takes for a network request to travel from $\omega_i$
to $\Gamma$, or vice versa. $K$ and $\Lambda$ describes the internal properties
of $\Gamma$. Let $K$ describe the event propagation model of $\Gamma$. It is
expressed as $K = \{ \kappa_0, \kappa_1 \}$ where $\kappa_0$ and $\kappa_1$
represents the event propagation model of the dispatcher and the event handler
respectively and $\kappa_i \in \{ \textit{serial}, \textit{preemptive},
\textit{cooperative} \}$. Let $\Lambda$ describe the resource demand in
$\Gamma$ from each event. It is expressed as $\Lambda = \{ \lambda_0, \lambda_1
\}$ where $\lambda_0$ and $\lambda_1$ represents the CPU intensity and the I/O
intensity induced by each event, where $0 \leq \lambda_i \leq 1$. Let $\chi$
describe the hardware platform of $\Gamma$, specifically the number of cores on
the CPU.

\begin{table}[h!]
    \caption{An overview of the internal and external properties of $\Gamma$.}
    \label{tab:gateway_config}

    \begin{center}
        \begin{tabular}{|l|l|}
            \hline
            Parameter   & Description \\
            \hline
            $q$         & Number of devices. \\
            $\phi$      & Frequency of generated events. \\
            $\delta$    & The delay, or latency, added to the network. \\
            $\kappa_0$  & Event propagation model of the dispatcher. \\
            $\kappa_1$  & Event propagation model of the event handler. \\
            $\lambda_0$ & CPU intensity induced by each event. \\
            $\lambda_1$ & I/O intensity induced by each event. \\
            $\chi$      & Number of CPU cores. \\
            \hline
        \end{tabular}
    \end{center}
\end{table}

In addition to the rather static expression of an abstract gateway $\Gamma$,
the function $E$ is provided:

\begin{equation}
E(\Gamma, t_{start}, t_{end}) \to M = \big[ \epsilon_0, \epsilon_1, ...,
\epsilon_{m-1} \big]^T
\end{equation}

By providing $E$ with $\Gamma$ and two points in time $t_{start}$ and $t_{end}$
where $t_{start} \leq t_{end}$, a vector $M$ of ordered events $\epsilon_j$ is
returned, where $\epsilon_j = [ t_0^j, t_1^j, t_2^j, t_3^j, t_4^j ]$. Note that
$M$ is ordered by $t_0^j$ and $m$ is the total number of events. Each
$\epsilon_j$ include five timestamps $t_k^j$ that represents parts of the event
lifecycle:

\begin{description}
\item[$t_0^j$] represents the point in time when $\epsilon_j$ was created in
    $\omega_i$.
\item[$t_1^j$] represents the point in time when $\epsilon_j$ left $\omega_i$.
\item[$t_2^j$] represents the point in time when $\epsilon_j$ arrived at
    $\Gamma$.
\item[$t_3^j$] represents the point in time when $\epsilon_j$ was dispatched to
    the event handler in $\Gamma$.
\item[$t_4^j$] represents the point in time when $\Gamma$ finished processing
    $\epsilon_j$.
\end{description}

These timestamps holds a set of basic constraints:

\begin{equation}
t_{start} \leq t_k^j
\end{equation}

\begin{equation} \label{eq:t_0}
t_0^j \leq t_{end}
\end{equation}

\begin{equation}
t_0^j \leq t_1^j \leq t_2^j \leq t_3^j \leq t_4^j
\end{equation}

Let $\Gamma_c$ be a specific gateway with the configuration $c$.  Running
$E(\Gamma_c, t_{start}, t_{end})$ returns the vector $M_c$, which contains the
events produced by $\Gamma_c$.

\subsection{Performance metrics}
\label{sec:performance_metrics}

With $M$ and $\epsilon_j$ it is possible to derive some performance metrics.
Throughput $T$ can be calculated as:

$$
T = \frac{\sum_{\epsilon_j \in M}{\pi(\epsilon_j)}}{t_{end} - t_{start}}
\text{ [events / s]}
$$

It is also possible to analyze the response time for an event, how long time it
spent in $\omega_i$ and how long time it spent in $\Gamma$. Let:

\begin{description}

    \item[$d_0^j = t_4^j - t_0^j$] be the response time of $\epsilon_j$, i.e. the
        total time between that the event was created until it was processed.

    \item[$d_1^j = t_1^j - t_0^j$] be the wait time, i.e. the time $\epsilon_j$
        spent in $\omega_i$.

    \item[$d_2^j = t_4^j - t_2^j$] be the process time, i.e. $\epsilon_j$ spent
        in $\Gamma$.

\end{description}

Given a vector of events $M_c$, with length $m$, created by running $E$ on a
gateway with the configuration $c$, let $D_k^c$ be a vector containing $\big[
    d_k^0, d_k^1, ..., d_k^{m-1} \big]$, where $k = 0, 1, 2$, $d_0^j$ is the
response time, $d_1^j$ is the wait time and $d_2^j$ is the process time for
$\epsilon_j \in M_c$. Then:

\begin{description}

    \item[$avg(D_k^c)$] is the average value of all $d_k^j \in D_k^c$.

\end{description}

\subsection{Analysing the load}
\label{sec:load}

A system is said to be \textit{at load} when the system is working at full
capacity and is not able to produce higher throughput. Due to the
strict time limitation provided by $t_{end}$ there might be cases where some
events $\epsilon_j$ has $t_k^j \geq t_{end}$. These are events that haven't
been entirely processed by $\Gamma$. Let $\pi$ be a function such that:

\[
    \pi(\epsilon_j) =
\begin{cases}
    1, & \text{if } t_4^j \leq t_{end} \\
    0, & \text{otherwise}
\end{cases}
\]

With $\pi$, the load on $\Gamma$ can be analysed. The ratio between created
events and processed events can give hint on how many events $\Gamma$ can
handle. Let $P$ denote the ratio between created and processed events:

$$
P = \frac{P_\pi}{m} = \frac{\sum_{\epsilon_j \in M}{\pi(\epsilon_j)}}{m}
$$

$P$ will always be in the interval $0 \leq P \leq 1$, because $t_4^j$ can be
greater than $t_{end}$, which means $\pi(\epsilon_j) = 0$, thus $m \geq P_\pi$.
In cases where $P \simeq 1$, $\Gamma$ is able to process most of the events and
it might even be so that $\Gamma$ is waiting for new events to be created. In
that case, we can not be sure whether $\Gamma$ is at a proper load or not.
However, in cases where $P \ll 1$ we know for sure $\Gamma$ is not able to
process all events, so there must be events on the device waiting for $\Gamma$.
The response time, $t_4^j - t_0^j$, must therefore grow for each new event that
is created. A proper load is therefore one where $P \simeq 1$, but where an
increase in total amount of events $m$ only reduces $P$.

\section{Approach}

The goal of this thesis is to map how different gateway architectures affect
its performance in different environments. Using the abstract gateway model
$\Gamma$, it is possible to derive the performance mapping. By running $E$ with
a set of different gateways $\Gamma_c$ and analyse the throughput, load,
response time, wait time and process time on the resulting vector $M_c$ the
performance mapping will be developed.

\subsection{Test environment}

\section{Development methodology}

This entire study will be conducted with Scrum as its backbone. Both the
writing of the thesis and the development of the applications to test will
happen in sprints. The project's backlog will initially be the fundamental
requirements of a master's thesis (based on requirements from Linköping
University) and the research questions. The forms of its user stories will
resemble traditional issues or requirements, but their scope can be wide and
their acceptance criteria abstract. Every week they undergo refinement and
abstract stories will be split into concrete ones as new knowledge about them
is acquired during the project. For instance, a starting user story (or issue)
will be \textit{"write the Results chapter"}. Initially this story is very
large, it is hard to do it in one sprint and it is hard to know where to start.
As time progress and the application to test have been developed and tests have
been conducted, the story can be split into more precise issues like
\textit{"present the developed application"} and \textit{"present a diagram
with the test results"}. These issues are (subjectively) easier to do in one
sprint and it is also easier to know when they are finished.

The author will take on all three traditional Scrum roles: product owner, Scrum
master and team member. Other stakeholders of the project are representatives
from Attentec, the examiner and a mentor from Linköping University. Each sprint
will have a length of 2 weeks and the sprint planning will, unlike the
traditional sprint planning \cite{sims2012scrum} where only the product owner
and team members are present, include a mentor from Attentec to help plan the
upcoming sprint. At the end of each sprint the current status of the project
will be presented to the stakeholders.

\section{Implementation}

The main approach this study will undertake to answer its question is to:

\begin{enumerate}
  \item find a state of the art reactive system in the industry with an
    appropiate level of complexity
  \item create a specification of the system
  \item reimplement the system with libuv
  \item create and conduct tests
  \item apply and evaluate the maintainability metrics on both systems
  \item present the results
\end{enumerate}

With help from Attentec, a multi-sensor monitor application used by one of
their clients will be specified and its source code will be used to compare its
software maintainability to a reimplementation with libuv. If no appropiate
system can be found in the industry, Attentec will aid in creating a
specification for a similar system as well as a state of the art technique to
implement it. The specification will follow the same pattern presented by Ardis
et al \cite{ardis1996framework}.

Even though libuv is written in C, C++ will be used as the main programming
language for the libuv implementation. With the dynamic programming style C++
offers with classes and templates, it will be more suitable for an
implementation that hopefully will attract web developers.

A testing environment will be created to simulate an IoT context where multiple
sensors are connected to the monitor application. A software testing suite will
also be setup to create unit tests.

\section{Evaluation}

Same software maintainability metrics will be applied on the system found in
the industry and the reimplementation of it. The results of the metrics will be
compared and presented.
